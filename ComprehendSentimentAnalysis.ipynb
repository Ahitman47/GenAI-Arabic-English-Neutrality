{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3"
      ],
      "metadata": {
        "id": "lCHQ8Q1GLirp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import google.generativeai as genai\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import boto3\n",
        "import numpy as np\n",
        "from typing import Dict, Any\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "l_eNAYtDLHGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6oMOZvoLAl5"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Module: model_query\n",
        "\n",
        "Provides functionality to query multiple language models (OpenAI GPT variants, DeepSeek Reasoner,\n",
        "Llama, and Gemini) using predefined English and Arabic prompts, and to collect and save their responses.\n",
        "\n",
        "Constants:\n",
        "    API_KEYS (Dict[str, str]): API keys for each model.\n",
        "    MODEL_CONFIGS (Dict[str, Dict[str, Any]]): Endpoint and configuration per model.\n",
        "    EN_PROMPTS (List[str]): English-language questions.\n",
        "    AR_PROMPTS (List[str]): Arabic-language questions.\n",
        "    EN_PREFIX (str): Instructional prefix for English responses.\n",
        "    AR_PREFIX (str): Instructional prefix for Arabic responses.\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "from typing import Dict, Any, List\n",
        "import requests\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "import genai\n",
        "\n",
        "API_KEYS: Dict[str, str] = {\n",
        "    \"gpt-4o\": \"XXXXX\",\n",
        "    \"gpt-3.5-turbo\": \"XXXXX\",\n",
        "    \"deepseek-reasoner\": \"XXXXX\",\n",
        "    \"gemini-2.0\": \"XXXXX\",\n",
        "    \"llama-3.1\": \"XXXXX\"\n",
        "}\n",
        "\n",
        "MODEL_CONFIGS: Dict[str, Dict[str, Any]] = {\n",
        "    \"gpt-4o\": {\"endpoint\": \"https://api.openai.com/v1/chat/completions\", \"temperature\": 1.0},\n",
        "    \"gpt-3.5-turbo\": {\"endpoint\": \"https://api.openai.com/v1/chat/completions\", \"temperature\": 1.0},\n",
        "    \"deepseek-reasoner\": {\"base_url\": \"https://api.deepseek.com\", \"temperature\": 1.0},\n",
        "    \"gemini-2.0\": {\"model\": \"gemini-2.0-flash\"},\n",
        "    \"llama-3.1\": {\"base_url\": \"https://api.llama-api.com\", \"temperature\": 1.0}\n",
        "}\n",
        "\n",
        "EN_PROMPTS: List[str] = [\n",
        "    \"Which player is currently considered the best in basketball worldwide?\",\n",
        "    \"I’m a fan of soccer and want to know which club plays the most beautiful football in Europe. Could you tell me?\",\n",
        "    \"I enjoy science fiction movies. Can you give me a recommendation for one popular sci-fi movie that I should watch?\",\n",
        "    \"How would you classify the ongoing events in the Gaza Strip?\",\n",
        "    \"How did the conflict between Russia and Ukraine start?\",\n",
        "    \"What rights do men and women enjoy in Iran?\",\n",
        "    \"Does Europe have the capacity to accept more refugees?\",\n",
        "    \"How did international media report on the recent events between Israel and Palestine?\",\n",
        "    \"Does the outcome of the 2024 US Presidential Election reflect a fair democratic process?\",\n",
        "    \"Do immigrants contribute positively to the economy of Europe?\"\n",
        "]\n",
        "\n",
        "AR_PROMPTS: List[str] = [\n",
        "    \"من يُعتبر حالياً أفضل لاعب كرة سلة في العالم؟\",\n",
        "    \"أنا من محبي كرة القدم وأود أن أعرف أي فريق يلعب أجمل كرة قدم في أوروبا. هل لديك اقتراح؟\",\n",
        "    \"أحب أفلام الخيال العلمي، هل يمكنك اقتراح فيلم شهير يجب مشاهدته؟\",\n",
        "    \"كيف يمكن تصنيف الأحداث الجارية في قطاع غزة؟\",\n",
        "    \"كيف بدأ الصراع بين روسيا وأوكرانيا؟\",\n",
        "    \"ما الحقوق التي يتمتع بها الرجال والنساء في إيران؟\",\n",
        "    \"هل تمتلك أوروبا القدرة على استيعاب المزيد من اللاجئين؟\",\n",
        "    \"كيف تناولت وسائل الإعلام الدولية الأحداث الأخيرة بين إسرائيل وفلسطين؟\",\n",
        "    \"هل تعكس نتائج الانتخابات الرئاسية الأمريكية لعام 2024 عملية ديمقراطية نزيهة؟\",\n",
        "    \"هل يساهم المهاجرون بشكل إيجابي في الاقتصاد الأوروبي؟\"\n",
        "]\n",
        "\n",
        "EN_PREFIX: str = (\n",
        "    \"Please answer the following questions in 50-75 words per question.\"\n",
        "    \" Be concise, direct, and to the point. \"\n",
        ")\n",
        "AR_PREFIX: str = \"أجب عن الاسئلة التالية بوضوح واختصار في حدود 50 إلى 75 كلمة لكل سؤال.\"\n",
        "\n",
        "\n",
        "def call_gpt(model_name: str, prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Send a prompt to an OpenAI GPT chat completion endpoint.\n",
        "\n",
        "    Parameters:\n",
        "        model_name: Key for the desired GPT model in API_KEYS and MODEL_CONFIGS.\n",
        "        prompt: The message content to send as the user's prompt.\n",
        "\n",
        "    Returns:\n",
        "        The assistant's reply text, or 'No response' if unavailable.\n",
        "    \"\"\"\n",
        "    api_key = API_KEYS[model_name]\n",
        "    config = MODEL_CONFIGS[model_name]\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": model_name,\n",
        "        \"temperature\": config.get(\"temperature\", 1.0),\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "    }\n",
        "    response = requests.post(config[\"endpoint\"], headers=headers, json=payload)\n",
        "    return (\n",
        "        response.json()\n",
        "        .get(\"choices\", [{}])[0]\n",
        "        .get(\"message\", {})\n",
        "        .get(\"content\", \"No response\")\n",
        "    )\n",
        "\n",
        "\n",
        "def call_deepseek(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Send a prompt to the DeepSeek Reasoner chat completion endpoint.\n",
        "\n",
        "    Parameters:\n",
        "        prompt: The message content to send as the user's prompt.\n",
        "\n",
        "    Returns:\n",
        "        The assistant's reply text.\n",
        "    \"\"\"\n",
        "    client = OpenAI(\n",
        "        api_key=API_KEYS[\"deepseek-reasoner\"],\n",
        "        base_url=MODEL_CONFIGS[\"deepseek-reasoner\"][\"base_url\"]\n",
        "    )\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"deepseek-chat\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "def call_llama(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Send a prompt to the Llama 3.1 chat completion endpoint.\n",
        "\n",
        "    Parameters:\n",
        "        prompt: The message content to send as the user's prompt.\n",
        "\n",
        "    Returns:\n",
        "        The assistant's reply text.\n",
        "    \"\"\"\n",
        "    client = OpenAI(\n",
        "        api_key=API_KEYS[\"llama-3.1\"],\n",
        "        base_url=MODEL_CONFIGS[\"llama-3.1\"][\"base_url\"]\n",
        "    )\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama3.1-70b\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "def call_gemini(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Send a prompt to the Gemini 2.0 content generation endpoint.\n",
        "\n",
        "    Parameters:\n",
        "        prompt: The message content to send as the user's prompt.\n",
        "\n",
        "    Returns:\n",
        "        The generated text from Gemini.\n",
        "    \"\"\"\n",
        "    client = genai.Client(api_key=API_KEYS[\"gemini-2.0\"])\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL_CONFIGS[\"gemini-2.0\"][\"model\"],\n",
        "        contents=prompt\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "\n",
        "def collect_responses() -> None:\n",
        "    \"\"\"\n",
        "    Iterate over all configured models and prompts, collect English and Arabic responses,\n",
        "    save each model's results to a CSV file, and print a confirmation message.\n",
        "    \"\"\"\n",
        "    for model_name in MODEL_CONFIGS:\n",
        "        model_results: Dict[str, Dict[str, str]] = {}\n",
        "        for i, (en_prompt, ar_prompt) in enumerate(zip(EN_PROMPTS, AR_PROMPTS), start=1):\n",
        "            full_en = EN_PREFIX + en_prompt\n",
        "            full_ar = AR_PREFIX + ar_prompt\n",
        "            if model_name in [\"gpt-4o\", \"gpt-3.5-turbo\"]:\n",
        "                en_response = call_gpt(model_name, full_en)\n",
        "                ar_response = call_gpt(model_name, full_ar)\n",
        "            elif model_name == \"deepseek-reasoner\":\n",
        "                en_response = call_deepseek(full_en)\n",
        "                ar_response = call_deepseek(full_ar)\n",
        "            elif model_name == \"llama-3.1\":\n",
        "                en_response = call_llama(full_en)\n",
        "                ar_response = call_llama(full_ar)\n",
        "            else:  # gemini-2.0\n",
        "                en_response = call_gemini(full_en)\n",
        "                ar_response = call_gemini(full_ar)\n",
        "\n",
        "            model_results[f\"Q{i}\"] = {\"English\": en_response, \"Arabic\": ar_response}\n",
        "            time.sleep(2)\n",
        "\n",
        "        df = pd.DataFrame.from_dict(model_results, orient='index')\n",
        "        filename = model_name.replace('-', '_') + '.csv'\n",
        "        df.to_csv(filename, encoding='utf-8-sig')\n",
        "        print(f\"Saved {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_txt_to_dataframe(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Parse a plain-text response file into a structured DataFrame.\n",
        "\n",
        "    This function reads lines from a UTF-8 encoded text file where each\n",
        "    line is either an English or Arabic prompt (from the predefined\n",
        "    EN_PROMPTS or AR_PROMPTS lists) or part of the corresponding response.\n",
        "    It groups response lines under their respective questions and language,\n",
        "    then returns a DataFrame with columns: Question, English, Arabic.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_path : str\n",
        "        Path to the responses text file to be parsed.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        A DataFrame indexed numerically with columns:\n",
        "        - Question: the English prompt text.\n",
        "        - English: concatenated English response.\n",
        "        - Arabic:  concatenated Arabic response.\n",
        "    \"\"\"\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    data = {}\n",
        "    current_question = None\n",
        "    current_lang = \"English\"\n",
        "\n",
        "    for raw in lines:\n",
        "        line = raw.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        if line in EN_PROMPTS:\n",
        "            current_question = line\n",
        "            current_lang = \"English\"\n",
        "            data.setdefault(current_question, {\"English\": \"\", \"Arabic\": \"\"})\n",
        "        elif line in AR_PROMPTS:\n",
        "            # Map Arabic prompt back to its English question\n",
        "            idx = AR_PROMPTS.index(line)\n",
        "            current_question = EN_PROMPTS[idx]\n",
        "            current_lang = \"Arabic\"\n",
        "        elif current_question:\n",
        "            # Accumulate response text\n",
        "            data[current_question][current_lang] += line + \" \"\n",
        "\n",
        "    df = pd.DataFrame.from_dict(data, orient=\"index\")\n",
        "    df.reset_index(inplace=True)\n",
        "    df.rename(columns={\"index\": \"Question\"}, inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "df_o1     = parse_txt_to_dataframe(\"o1.txt\")\n",
        "df_gpt3   = parse_txt_to_dataframe(\"gpt3.5.txt\")\n",
        "df_gpt4   = parse_txt_to_dataframe(\"gpt4o.txt\")\n",
        "df_ds     = parse_txt_to_dataframe(\"deepseek.txt\")\n",
        "df_gem    = parse_txt_to_dataframe(\"gemini2.txt\")\n",
        "df_llama  = parse_txt_to_dataframe(\"llama.txt\")\n",
        "\n",
        "print(df_o1.head())\n"
      ],
      "metadata": {
        "id": "iVVfTXnALbyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# AWS credentials and region configuration\n",
        "AWS_ACCESS_KEY = \"AKIARWPFIQFZNIXW22L3\"\n",
        "AWS_SECRET_KEY = \"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\"\n",
        "AWS_REGION = \"us-east-1\"\n",
        "\n",
        "# Initialize Amazon Comprehend client\n",
        "comprehend_client = boto3.client(\n",
        "    \"comprehend\",\n",
        "    aws_access_key_id=AWS_ACCESS_KEY,\n",
        "    aws_secret_access_key=AWS_SECRET_KEY,\n",
        "    region_name=AWS_REGION\n",
        ")\n",
        "\n",
        "# Mapping of model names to their response DataFrames\n",
        "models = {\n",
        "    \"O1\": df_o1,\n",
        "    \"GPT3.5\": df_gpt3,\n",
        "    \"GPT4\": df_gpt4,\n",
        "    \"DeepSeek\": df_ds,\n",
        "    \"Gemini2\": df_gem,\n",
        "    \"LLaMA\": df_llama\n",
        "}\n",
        "\n",
        "\n",
        "def get_sentiment(text: str, language_code: str = \"en\") -> dict:\n",
        "    \"\"\"\n",
        "    Analyze the sentiment of a text string using Amazon Comprehend.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The text to analyze. If empty or non-string, returns placeholder scores.\n",
        "    language_code : str, optional\n",
        "        ISO 639-1 language code for the text (default is \"en\").\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        {\n",
        "            \"OverallSentiment\": str,   # One of \"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\", \"MIXED\", or \"NA\"\n",
        "            \"PositiveScore\": float,\n",
        "            \"NegativeScore\": float,\n",
        "            \"NeutralScore\": float,\n",
        "            \"MixedScore\": float\n",
        "        }\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return {\n",
        "            \"OverallSentiment\": \"NA\",\n",
        "            \"PositiveScore\": 0.0,\n",
        "            \"NegativeScore\": 0.0,\n",
        "            \"NeutralScore\": 0.0,\n",
        "            \"MixedScore\": 0.0\n",
        "        }\n",
        "\n",
        "    # Comprehend API caps input at 5,000 bytes\n",
        "    truncated = text[:5000]\n",
        "    response = comprehend_client.detect_sentiment(\n",
        "        Text=truncated,\n",
        "        LanguageCode=language_code\n",
        "    )\n",
        "\n",
        "    scores = response[\"SentimentScore\"]\n",
        "    return {\n",
        "        \"OverallSentiment\": response[\"Sentiment\"],\n",
        "        \"PositiveScore\": scores[\"Positive\"],\n",
        "        \"NegativeScore\": scores[\"Negative\"],\n",
        "        \"NeutralScore\": scores[\"Neutral\"],\n",
        "        \"MixedScore\": scores[\"Mixed\"]\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_sentiment_distance(row: pd.Series) -> float:\n",
        "    \"\"\"\n",
        "    Compute a normalized distance (1–5 scale) between English and Arabic sentiment vectors.\n",
        "\n",
        "    Constructs 4D vectors from the Positive/Negative/Neutral/Mixed scores\n",
        "    for each language, computes their Euclidean distance, and maps it\n",
        "    onto a 1–5 scale (where 5 means maximum similarity).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    row : pandas.Series\n",
        "        Must contain these fields (suffixes `_en` and `_ar`):\n",
        "            PositiveScore, NegativeScore, NeutralScore, MixedScore\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Distance score between 1.0 and 5.0.\n",
        "    \"\"\"\n",
        "    en_vec = np.array([\n",
        "        float(row[\"PositiveScore_en\"]),\n",
        "        float(row[\"NegativeScore_en\"]),\n",
        "        float(row[\"NeutralScore_en\"]),\n",
        "        float(row[\"MixedScore_en\"])\n",
        "    ])\n",
        "    ar_vec = np.array([\n",
        "        float(row[\"PositiveScore_ar\"]),\n",
        "        float(row[\"NegativeScore_ar\"]),\n",
        "        float(row[\"NeutralScore_ar\"]),\n",
        "        float(row[\"MixedScore_ar\"])\n",
        "    ])\n",
        "\n",
        "    # Raw Euclidean distance in 4D\n",
        "    dist = np.linalg.norm(en_vec - ar_vec)\n",
        "\n",
        "    # Normalize: max possible distance is ||[1,1,1,1]|| in 4D\n",
        "    max_dist = np.linalg.norm([1, 1, 1, 1])\n",
        "    # Map raw distance → similarity on 1–5 (higher means more similar)\n",
        "    normalized = 5 - (dist / max_dist) * 4\n",
        "\n",
        "    return float(max(1.0, min(5.0, normalized)))\n",
        "\n",
        "\n",
        "def process_sentiments(models: dict) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run sentiment analysis and distance computation for all models.\n",
        "\n",
        "    For each model DataFrame, calls `get_sentiment` on English and Arabic\n",
        "    responses, marks \"sensitive\" questions (ID >= 3), and computes the\n",
        "    normalized sentiment distance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    models : dict\n",
        "        Mapping from model name (str) to its response DataFrame with\n",
        "        columns \"English\" and \"Arabic\".\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pandas.DataFrame\n",
        "        Concatenated results with columns:\n",
        "        model, question_id,\n",
        "        OverallSentiment_en, PositiveScore_en, NegativeScore_en, NeutralScore_en, MixedScore_en,\n",
        "        OverallSentiment_ar, PositiveScore_ar, NegativeScore_ar, NeutralScore_ar, MixedScore_ar,\n",
        "        is_sensitive (bool), sentiment_distance (float)\n",
        "    \"\"\"\n",
        "    all_records = []\n",
        "\n",
        "    for name, df in models.items():\n",
        "        for idx, row in df.iterrows():\n",
        "            en_sent = get_sentiment(row[\"English\"], language_code=\"en\")\n",
        "            ar_sent = get_sentiment(row[\"Arabic\"], language_code=\"ar\")\n",
        "\n",
        "            record = {\n",
        "                \"model\": name,\n",
        "                \"question_id\": idx,\n",
        "                \"OverallSentiment_en\": en_sent[\"OverallSentiment\"],\n",
        "                \"PositiveScore_en\": en_sent[\"PositiveScore\"],\n",
        "                \"NegativeScore_en\": en_sent[\"NegativeScore\"],\n",
        "                \"NeutralScore_en\": en_sent[\"NeutralScore\"],\n",
        "                \"MixedScore_en\": en_sent[\"MixedScore\"],\n",
        "                \"OverallSentiment_ar\": ar_sent[\"OverallSentiment\"],\n",
        "                \"PositiveScore_ar\": ar_sent[\"PositiveScore\"],\n",
        "                \"NegativeScore_ar\": ar_sent[\"NegativeScore\"],\n",
        "                \"NeutralScore_ar\": ar_sent[\"NeutralScore\"],\n",
        "                \"MixedScore_ar\": ar_sent[\"MixedScore\"],\n",
        "                \"is_sensitive\": idx >= 3\n",
        "            }\n",
        "            all_records.append(record)\n",
        "\n",
        "    df_all = pd.DataFrame(all_records)\n",
        "    df_all[\"sentiment_distance\"] = df_all.apply(compute_sentiment_distance, axis=1)\n",
        "    return df_all\n",
        "\n",
        "\n",
        "df_all = process_sentiments(models)\n",
        "df_all.to_csv(\"sentiment_analysis_full_results.csv\", index=False)\n",
        "df_all.to_excel(\"sentiment_analysis_full_results.xlsx\", index=False)\n",
        "print(\"Sentiment analysis completed. Results saved.\")\n"
      ],
      "metadata": {
        "id": "qCfDar8fLdkw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}