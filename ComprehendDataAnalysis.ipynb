{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_swsxH0nNFLy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import math\n",
        "import itertools\n",
        "from scipy.stats import (\n",
        "    ttest_rel,      # Paired t-test\n",
        "    ttest_ind,      # Independent two-sample t-test\n",
        "    f_oneway,       # One-way ANOVA\n",
        "    mannwhitneyu,   # Non-parametric two-sample\n",
        "    wilcoxon,       # Non-parametric paired\n",
        "    kruskal,        # Non-parametric version of one-way ANOVA\n",
        "    ttest_1samp\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load and prepare sentiment analysis results.\n",
        "\n",
        "Reads raw sentiment results from an Excel file, computes the Euclidean distance\n",
        "between Arabic and English sentiment vectors, calculates per-dimension differences,\n",
        "and prints a quick preview of the processed DataFrame and its column names.\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_excel(\"sentiment_analysis_full_results raw.xlsx\")\n",
        "\n",
        "# Compute Euclidean distance (L2 norm) across the four sentiment score dimensions\n",
        "df[\"sentiment_diff\"] = np.sqrt(\n",
        "    (df[\"PositiveScore_ar\"] - df[\"PositiveScore_en\"]) ** 2 +\n",
        "    (df[\"NegativeScore_ar\"] - df[\"NegativeScore_en\"]) ** 2 +\n",
        "    (df[\"NeutralScore_ar\"]  - df[\"NeutralScore_en\"]) ** 2 +\n",
        "    (df[\"MixedScore_ar\"]    - df[\"MixedScore_en\"]) ** 2\n",
        ")\n",
        "\n",
        "# Calculate directional differences for each sentiment dimension\n",
        "df[\"diff_positive\"] = df[\"PositiveScore_en\"] - df[\"PositiveScore_ar\"]\n",
        "df[\"diff_negative\"] = df[\"NegativeScore_en\"] - df[\"NegativeScore_ar\"]\n",
        "df[\"diff_neutral\"]  = df[\"NeutralScore_en\"]  - df[\"NeutralScore_ar\"]\n",
        "df[\"diff_mixed\"]    = df[\"MixedScore_en\"]    - df[\"MixedScore_ar\"]\n",
        "\n",
        "# Display a sample of the processed data and the list of columns\n",
        "print(\"Data sample:\\n\", df.head())\n",
        "print(\"\\nColumns:\\n\", df.columns)\n"
      ],
      "metadata": {
        "id": "w9_0ttQNPJMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Compute and display mean Euclidean sentiment distances across different\n",
        "groupings: by model & question, by question alone, by model & sensitivity flag,\n",
        "by sensitivity flag alone, and by model alone.\n",
        "\"\"\"\n",
        "\n",
        "mean_scores = df.groupby([\"model\", \"question_id\"])[\"sentiment_diff\"].mean()\n",
        "print(\"\\n=== ITEM 1: Mean scores per LLM & question ===\")\n",
        "print(mean_scores)\n",
        "\n",
        "mean_scores_questions = df.groupby(\"question_id\")[\"sentiment_diff\"].mean()\n",
        "print(\"\\n=== ITEM 1: Mean scores per question ===\")\n",
        "print(mean_scores_questions)\n",
        "\n",
        "mean_scores_sens_llm = df.groupby([\"model\", \"is_sensitive\"])[\"sentiment_diff\"].mean()\n",
        "print(\"\\n=== ITEM 1: Mean scores by LLM & Sensitive ===\")\n",
        "print(mean_scores_sens_llm)\n",
        "\n",
        "mean_scores_sens = df.groupby(\"is_sensitive\")[\"sentiment_diff\"].mean()\n",
        "print(\"\\n=== ITEM 1: Mean scores by Sensitive ===\")\n",
        "print(mean_scores_sens)\n",
        "\n",
        "mean_scores_model = df.groupby(\"model\")[\"sentiment_diff\"].mean()\n",
        "print(\"\\n=== ITEM 1: Mean scores by model ===\")\n",
        "print(mean_scores_model)\n"
      ],
      "metadata": {
        "id": "MQ-E1_O4PdQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Configure plotting and generate descriptive visualizations of AR–EN sentiment differences.\n",
        "\n",
        "This script performs the following steps:\n",
        "1. Sets up Matplotlib aesthetics using the Aptos font.\n",
        "2. Defines `compute_stats` to aggregate sentiment difference statistics.\n",
        "3. Computes statistics grouped by various dimensions.\n",
        "4. Produces four plots:\n",
        "   - Mean sentiment difference by model & question (with overall mean background).\n",
        "   - Mean sentiment difference by model & topic sensitivity.\n",
        "   - Mean sentiment difference by topic sensitivity.\n",
        "   - Mean sentiment difference by model.\n",
        "\"\"\"\n",
        "\n",
        "mpl.rcParams[\"font.family\"] = \"sans-serif\"\n",
        "mpl.rcParams[\"font.sans-serif\"] = [\"Aptos\", \"DejaVu Sans\"]  # Fallback fonts\n",
        "mpl.rcParams[\"axes.titleweight\"] = \"bold\"\n",
        "mpl.rcParams[\"axes.titlesize\"] = 14\n",
        "mpl.rcParams[\"axes.labelsize\"] = 12\n",
        "mpl.rcParams[\"xtick.labelsize\"] = 11\n",
        "mpl.rcParams[\"ytick.labelsize\"] = 11\n",
        "mpl.rcParams[\"legend.fontsize\"] = 11\n",
        "\n",
        "\n",
        "def compute_stats(df: pd.DataFrame, group_cols: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Group by the specified columns and compute descriptive stats for 'sentiment_diff'.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame containing a 'sentiment_diff' column and the grouping columns.\n",
        "    group_cols : list of str\n",
        "        The column names to group by.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pandas.DataFrame\n",
        "        An aggregated DataFrame with columns:\n",
        "        - mean : float, mean of sentiment_diff\n",
        "        - std  : float, standard deviation\n",
        "        - count: int, number of observations\n",
        "        - sem  : float, standard error of the mean\n",
        "    \"\"\"\n",
        "    stats = df.groupby(group_cols)[\"sentiment_diff\"].agg([\"mean\", \"std\", \"count\"])\n",
        "    stats[\"sem\"] = stats[\"std\"] / np.sqrt(stats[\"count\"])\n",
        "    return stats\n",
        "\n",
        "\n",
        "stats_model_q    = compute_stats(df, [\"model\", \"question_id\"])\n",
        "stats_model_sens = compute_stats(df, [\"model\", \"is_sensitive\"])\n",
        "stats_sens       = compute_stats(df, [\"is_sensitive\"])\n",
        "stats_model      = compute_stats(df, [\"model\"])\n",
        "\n",
        "\n",
        "overall_mean = pivot_mean.mean(axis=1)\n",
        "x_positions = np.arange(len(pivot_mean.index))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "ax = plt.gca()\n",
        "\n",
        "ax.bar(\n",
        "    x_positions,\n",
        "    overall_mean,\n",
        "    width=0.8,\n",
        "    alpha=0.3,\n",
        "    label=\"Overall Mean\",\n",
        "    zorder=0\n",
        ")\n",
        "\n",
        "pivot_mean.plot(\n",
        "    kind=\"bar\",\n",
        "    yerr=pivot_sem,\n",
        "    edgecolor=\"black\",\n",
        "    capsize=4,\n",
        "    ax=ax,\n",
        "    zorder=1\n",
        ")\n",
        "\n",
        "plt.title(\"AR–EN Sentiment Difference by LLM and Question\")\n",
        "plt.xlabel(\"Question ID\")\n",
        "plt.ylabel(\"Sentiment Difference\")\n",
        "plt.xticks(ticks=x_positions, labels=pivot_mean.index, rotation=45, ha=\"right\")\n",
        "plt.legend(title=\"LLM / Overall Mean\", bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
        "plt.grid(axis=\"y\", alpha=0.4)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "pivot_mean_ms = stats_model_sens[\"mean\"].reset_index().pivot(\n",
        "    index=\"model\",\n",
        "    columns=\"is_sensitive\",\n",
        "    values=\"mean\"\n",
        ")\n",
        "pivot_sem_ms = stats_model_sens[\"sem\"].reset_index().pivot(\n",
        "    index=\"model\",\n",
        "    columns=\"is_sensitive\",\n",
        "    values=\"sem\"\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(9, 5))\n",
        "ax = pivot_mean_ms.plot(\n",
        "    kind=\"bar\",\n",
        "    yerr=pivot_sem_ms,\n",
        "    edgecolor=\"black\",\n",
        "    capsize=4,\n",
        "    ax=plt.gca()\n",
        ")\n",
        "\n",
        "plt.title(\"Mean AR–EN Sentiment Difference by LLM and Topic Sensitivity\")\n",
        "plt.xlabel(\"LLM (Model)\")\n",
        "plt.ylabel(\"Mean Sentiment Difference ± SEM\")\n",
        "plt.legend([\"Non-sensitive\", \"Sensitive\"], title=\"Topic\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.grid(axis=\"y\", alpha=0.4)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "df_sens_plot = stats_sens.reset_index()\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.bar(\n",
        "    x=[\"Non-sensitive\", \"Sensitive\"],\n",
        "    height=df_sens_plot[\"mean\"],\n",
        "    yerr=df_sens_plot[\"sem\"],\n",
        "    edgecolor=\"black\",\n",
        "    capsize=4\n",
        ")\n",
        "\n",
        "plt.title(\"Mean AR–EN Sentiment Difference by Topic Sensitivity\")\n",
        "plt.xlabel(\"Topic Sensitivity\")\n",
        "plt.ylabel(\"Mean Sentiment Difference ± SEM\")\n",
        "plt.grid(axis=\"y\", alpha=0.4)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "df_model_plot = stats_model.reset_index().sort_values(\"mean\", ascending=False)\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.barh(\n",
        "    y=df_model_plot[\"model\"],\n",
        "    width=df_model_plot[\"mean\"],\n",
        "    xerr=df_model_plot[\"sem\"],\n",
        "    edgecolor=\"black\",\n",
        "    capsize=4\n",
        ")\n",
        "\n",
        "plt.title(\"Mean AR–EN Sentiment Difference by LLM\")\n",
        "plt.xlabel(\"Mean Sentiment Difference ± SEM\")\n",
        "plt.ylabel(\"LLM (Model)\")\n",
        "plt.grid(axis=\"x\", alpha=0.4)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h2g5IErOPl29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This compares the distributions of 'sentiment_diff' for sensitive\n",
        "(is_sensitive == 1) versus non-sensitive (is_sensitive == 0) questions\n",
        "across all models. It uses a two-sided Mann–Whitney U test to determine\n",
        "if the difference between the two groups is statistically significant.\n",
        "If either group has fewer than two samples, it reports insufficient data.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n=== Compare Sensitive vs Non-Sensitive across ALL LLMs ===\")\n",
        "\n",
        "sens = df[df[\"is_sensitive\"] == 1][\"sentiment_diff\"]\n",
        "nonsens = df[df[\"is_sensitive\"] == 0][\"sentiment_diff\"]\n",
        "\n",
        "if len(sens) < 2 or len(nonsens) < 2:\n",
        "    print(\"Not enough data to test sensitive vs non-sensitive across all LLMs.\")\n",
        "else:\n",
        "    stat, pval = mannwhitneyu(sens, nonsens, alternative='two-sided')\n",
        "    print(f\"Mann-Whitney (All LLMs, Sens vs. Non-Sens), p={pval:.4f}\")\n"
      ],
      "metadata": {
        "id": "fO_OnQ4nP5Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "For each language model (LLM), this section compares the distributions of\n",
        "'sentiment_diff' for sensitive (is_sensitive == 1) versus non-sensitive\n",
        "(is_sensitive == 0) questions using a two-sided Mann–Whitney U test.\n",
        "If either subgroup has fewer than two observations, it reports insufficient data.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n=== ITEM 2: Compare Sensitive vs Non-Sensitive (sentiment_diff) per LLM ===\")\n",
        "llms = df[\"model\"].unique()\n",
        "\n",
        "for llm in llms:\n",
        "    subset = df[df[\"model\"] == llm]\n",
        "    sens = subset[subset[\"is_sensitive\"] == 1][\"sentiment_diff\"]\n",
        "    nonsens = subset[subset[\"is_sensitive\"] == 0][\"sentiment_diff\"]\n",
        "\n",
        "    # Ensure at least two data points in each group for the test\n",
        "    if len(sens) < 2 or len(nonsens) < 2:\n",
        "        print(f\"LLM={llm}: Not enough data to test sensitive vs non-sensitive.\")\n",
        "        continue\n",
        "\n",
        "    # Perform two-sided Mann–Whitney U test\n",
        "    stat, pval = mannwhitneyu(sens, nonsens, alternative='two-sided')\n",
        "    print(f\"LLM={llm}, Mann-Whitney U (sentiment_diff, Sens vs Non-sens): p={pval:.4f}\")\n"
      ],
      "metadata": {
        "id": "eNImsAz4QMz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Compare sentiment differences across language models.\n",
        "\n",
        "This code:\n",
        "- Calculates summary stats (mean, std, count) of 'sentiment_diff' by model.\n",
        "- Runs one-way ANOVA and Kruskal–Wallis tests across models that have >1 sample.\n",
        "\"\"\"\n",
        "\n",
        "summary = df.groupby(\"model\")[\"sentiment_diff\"].agg([\"mean\", \"std\", \"count\"])\n",
        "print(summary)\n",
        "\n",
        "print(\"\\n=== Compare sentiment_diff across LLMs (ANOVA & Kruskal-Wallis) ===\")\n",
        "groups = []\n",
        "models_for_test = []\n",
        "for llm in llms:\n",
        "    scores = df.loc[df[\"model\"] == llm, \"sentiment_diff\"].dropna()\n",
        "    if len(scores) > 1:\n",
        "        groups.append(scores)\n",
        "        models_for_test.append(llm)\n",
        "\n",
        "if len(groups) >= 2:\n",
        "    f_stat, p_val = f_oneway(*groups)\n",
        "    print(f\"One-way ANOVA (sentiment_diff) across LLMs: F={f_stat:.4f}, p={p_val:.4f}\")\n",
        "    k_stat, k_p = kruskal(*groups)\n",
        "    print(f\"Kruskal-Wallis (sentiment_diff) across LLMs: H={k_stat:.4f}, p={k_p:.4f}\")\n",
        "else:\n",
        "    print(\"Not enough models with >1 observation for statistical tests.\")\n",
        "\n",
        "dims = [\"diff_positive\", \"diff_negative\", \"diff_mixed\", \"diff_neutral\"]\n",
        "for col in dims:\n",
        "    data = df[col].dropna()\n",
        "    t_stat, p_val = ttest_1samp(data, 0)\n",
        "    print(f\"{col}: t={t_stat:.4f}, p={p_val:.4f} (one-sample t-test against zero)\")\n",
        "\n"
      ],
      "metadata": {
        "id": "f_6KkdrGQOwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Analyze per-dimension sentiment differences and perform one-sample t-tests.\n",
        "\n",
        "`df` contains:\n",
        "- PositiveScore_ar, PositiveScore_en\n",
        "- NegativeScore_ar, NegativeScore_en\n",
        "- MixedScore_ar,    MixedScore_en\n",
        "- NeutralScore_ar,  NeutralScore_en\n",
        "- is_sensitive (0 or 1)\n",
        "\n",
        "For each sentiment dimension, this script:\n",
        "1. Prints mean Arabic vs. English scores and their overall mean difference.\n",
        "2. Runs a one-sample t-test on (EN – AR) for all rows.\n",
        "3. Runs the same t-test separately for non-sensitive (0) and sensitive (1) subsets.\n",
        "\"\"\"\n",
        "\n",
        "base_dims = [\"PositiveScore\", \"NegativeScore\", \"MixedScore\", \"NeutralScore\"]\n",
        "\n",
        "for dim in base_dims:\n",
        "    col_ar = f\"{dim}_ar\"\n",
        "    col_en = f\"{dim}_en\"\n",
        "\n",
        "    mean_ar = df[col_ar].mean()\n",
        "    mean_en = df[col_en].mean()\n",
        "    diff_col = df[col_en] - df[col_ar]\n",
        "\n",
        "    print(f\"\\n=== {dim} ===\")\n",
        "    print(\n",
        "        f\"Mean AR: {mean_ar:.4f}, \"\n",
        "        f\"Mean EN: {mean_en:.4f}, \"\n",
        "        f\"Overall mean diff: {diff_col.mean():.4f}\"\n",
        "    )\n",
        "\n",
        "    diffs = diff_col.dropna()\n",
        "    if len(diffs) > 1:\n",
        "        t_stat, p_val = ttest_1samp(diffs, 0)\n",
        "        print(f\"Overall t-test: t={t_stat:.4f}, p={p_val:.4f}\")\n",
        "    else:\n",
        "        print(\"Overall t-test: not enough data.\")\n",
        "\n",
        "    for cat in (0, 1):\n",
        "        cat_diffs = (df[df[\"is_sensitive\"] == cat][col_en] -\n",
        "                     df[df[\"is_sensitive\"] == cat][col_ar]).dropna()\n",
        "        if len(cat_diffs) > 1:\n",
        "            t_stat_cat, p_val_cat = ttest_1samp(cat_diffs, 0)\n",
        "            print(\n",
        "                f\"  is_sensitive={cat}: mean diff={cat_diffs.mean():.4f}, \"\n",
        "                f\"t={t_stat_cat:.4f}, p={p_val_cat:.4f}\"\n",
        "            )\n",
        "        else:\n",
        "            print(f\"  is_sensitive={cat}: not enough data.\")\n"
      ],
      "metadata": {
        "id": "kqqAz6mYQfxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Plot mean Arabic vs. English sentiment scores by dimension and topic sensitivity.\n",
        "\n",
        "For each sentiment dimension (Positive, Negative, Mixed, Neutral), this script:\n",
        "1. Computes mean scores for Arabic and English in non-sensitive and sensitive subsets.\n",
        "2. Performs a paired t-test (EN vs. AR) within each subset when possible.\n",
        "3. Stores the means and p-values in a data structure.\n",
        "4. Creates a 2×2 grid of bar plots showing the mean scores for each dimension,\n",
        "   with separate bars for non-sensitive and sensitive topics.\n",
        "\"\"\"\n",
        "\n",
        "dimensions = [\"PositiveScore\", \"NegativeScore\", \"MixedScore\", \"NeutralScore\"]\n",
        "dim_labels = [\"Positive\", \"Negative\", \"Mixed\", \"Neutral\"]\n",
        "\n",
        "ar_color = \"#1f77b4\"\n",
        "en_color = \"#ff7f0e\"\n",
        "\n",
        "data_store = {}\n",
        "\n",
        "for dim in dimensions:\n",
        "    col_ar = f\"{dim}_ar\"\n",
        "    col_en = f\"{dim}_en\"\n",
        "\n",
        "    sub_non = df[df[\"is_sensitive\"] == 0].dropna(subset=[col_ar, col_en])\n",
        "    if not sub_non.empty:\n",
        "        mean_ar_non = sub_non[col_ar].mean()\n",
        "        mean_en_non = sub_non[col_en].mean()\n",
        "        if len(sub_non) > 1:\n",
        "            _, p_non = ttest_rel(sub_non[col_en], sub_non[col_ar])\n",
        "        else:\n",
        "            p_non = math.nan\n",
        "    else:\n",
        "        mean_ar_non, mean_en_non, p_non = 0.0, 0.0, math.nan\n",
        "\n",
        "    sub_sens = df[df[\"is_sensitive\"] == 1].dropna(subset=[col_ar, col_en])\n",
        "    if not sub_sens.empty:\n",
        "        mean_ar_sens = sub_sens[col_ar].mean()\n",
        "        mean_en_sens = sub_sens[col_en].mean()\n",
        "        if len(sub_sens) > 1:\n",
        "            _, p_sens = ttest_rel(sub_sens[col_en], sub_sens[col_ar])\n",
        "        else:\n",
        "            p_sens = math.nan\n",
        "    else:\n",
        "        mean_ar_sens, mean_en_sens, p_sens = 0.0, 0.0, math.nan\n",
        "\n",
        "    data_store[dim] = (\n",
        "        mean_ar_non, mean_en_non, p_non,\n",
        "        mean_ar_sens, mean_en_sens, p_sens\n",
        "    )\n",
        "\n",
        "def label_bar(rects, value):\n",
        "    \"\"\"\n",
        "    Place a centered label within the bar showing its value.\n",
        "    \"\"\"\n",
        "    rect = rects[0]\n",
        "    x_center = rect.get_x() + rect.get_width() / 2\n",
        "    y_center = value * 0.5\n",
        "    plt.text(x_center, y_center, f\"{value:.2f}\",\n",
        "             ha=\"center\", va=\"center\", fontsize=8)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 8), sharey=False)\n",
        "fig.suptitle(\"Mean Arabic vs. English Sentiment Scores by Dimension\", fontsize=12, y=0.98)\n",
        "\n",
        "for i, dim in enumerate(dimensions):\n",
        "    ax = axes[i // 2, i % 2]\n",
        "    (mean_ar_non, mean_en_non, p_non,\n",
        "     mean_ar_sens, mean_en_sens, p_sens) = data_store[dim]\n",
        "\n",
        "    positions = [0, 1]\n",
        "    width = 0.35\n",
        "\n",
        "    offsets = (-width/2, width/2)\n",
        "    coords = {\n",
        "        \"ar_non\": positions[0] + offsets[0],\n",
        "        \"en_non\": positions[0] + offsets[1],\n",
        "        \"ar_sens\": positions[1] + offsets[0],\n",
        "        \"en_sens\": positions[1] + offsets[1]\n",
        "    }\n",
        "\n",
        "    bars = {\n",
        "        \"ar_non\": ax.bar(coords[\"ar_non\"], mean_ar_non, width, color=ar_color, edgecolor=\"black\"),\n",
        "        \"en_non\": ax.bar(coords[\"en_non\"], mean_en_non, width, color=en_color, edgecolor=\"black\"),\n",
        "        \"ar_sens\": ax.bar(coords[\"ar_sens\"], mean_ar_sens, width, color=ar_color, edgecolor=\"black\"),\n",
        "        \"en_sens\": ax.bar(coords[\"en_sens\"], mean_en_sens, width, color=en_color, edgecolor=\"black\")\n",
        "    }\n",
        "\n",
        "    label_bar(bars[\"ar_non\"], mean_ar_non)\n",
        "    label_bar(bars[\"en_non\"], mean_en_non)\n",
        "    label_bar(bars[\"ar_sens\"], mean_ar_sens)\n",
        "    label_bar(bars[\"en_sens\"], mean_en_sens)\n",
        "\n",
        "    values = [mean_ar_non, mean_en_non, mean_ar_sens, mean_en_sens]\n",
        "    max_val = max(values) if any(values) else 1.0\n",
        "    ax.set_ylim(0, max_val * 1.2)\n",
        "\n",
        "    ax.set_xticks(positions)\n",
        "    ax.set_xticklabels([\"Non-Sensitive\", \"Sensitive\"])\n",
        "    ax.set_title(dim_labels[i])\n",
        "    ax.set_ylabel(\"Mean Score\")\n",
        "\n",
        "handles = [\n",
        "    plt.Rectangle((0, 0), 1, 1, color=ar_color, label=\"Arabic\"),\n",
        "    plt.Rectangle((0, 0), 1, 1, color=en_color, label=\"English\")\n",
        "]\n",
        "fig.legend(handles=handles, loc=\"upper right\", fontsize=9)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M6uAoQjCRBRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Run one-sample t-tests on per-model difference metrics.\n",
        "\n",
        "For each language model and each difference column\n",
        "('diff_positive', 'diff_negative', 'diff_mixed', 'diff_neutral'), this code:\n",
        "1. Extracts the non-null values for that model and dimension.\n",
        "2. If there are at least two observations, performs a one-sample t-test\n",
        "   comparing the mean difference against zero.\n",
        "3. Prints the model name, dimension, observed mean, and p-value.\n",
        "\"\"\"\n",
        "\n",
        "import itertools\n",
        "from scipy.stats import ttest_1samp\n",
        "\n",
        "llms = df[\"model\"].unique()\n",
        "dims = [\"diff_positive\", \"diff_negative\", \"diff_mixed\", \"diff_neutral\"]\n",
        "\n",
        "for llm, dim in itertools.product(llms, dims):\n",
        "    values = df[df[\"model\"] == llm][dim].dropna()\n",
        "    if len(values) < 2:\n",
        "        continue\n",
        "\n",
        "    t_stat, p_val = ttest_1samp(values, 0)\n",
        "    print(f\"LLM={llm}, dimension={dim}: mean={values.mean():.4f}, p={p_val:.4f}\")\n"
      ],
      "metadata": {
        "id": "VviexcEERqVy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}