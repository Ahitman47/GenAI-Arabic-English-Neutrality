{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYT8cjNxbmFz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Module to fetch, sample, and save article titles from the Perigon API.\n",
        "\n",
        "Defines:\n",
        "- fetch_titles: retrieve titles with pagination and rate-limit handling.\n",
        "- print_sample_titles: display a subset of fetched titles.\n",
        "- save_titles_to_csv: write titles to a CSV file.\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "API_KEY = \"<YOUR_API_KEY_HERE>\"\n",
        "BASE_URL = \"https://api.goperigon.com/v1/all\"\n",
        "\n",
        "def fetch_titles(from_date: str, to_date: str, source: str = \"cnn.com\",\n",
        "                 max_pages: int = 30, retry_after: int = 5) -> list:\n",
        "    \"\"\"\n",
        "    Fetch article titles from a news source over a date range.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    from_date : str\n",
        "        Start date (YYYY-MM-DD) for fetching articles.\n",
        "    to_date : str\n",
        "        End date (YYYY-MM-DD) for fetching articles.\n",
        "    source : str, default \"cnn.com\"\n",
        "        Domain of the news source to query.\n",
        "    max_pages : int, default 30\n",
        "        Maximum number of result pages to request.\n",
        "    retry_after : int, default 5\n",
        "        Seconds to wait after a 429 (rate limit) response before retrying.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list of str\n",
        "        Titles of articles retrieved from the API.\n",
        "    \"\"\"\n",
        "    titles = []\n",
        "    page = 1\n",
        "\n",
        "    while page <= max_pages:\n",
        "        url = (\n",
        "            f\"{BASE_URL}?source={source}\"\n",
        "            f\"&from={from_date}&to={to_date}&sortBy=date\"\n",
        "            f\"&page={page}&apiKey={API_KEY}&pageSize=100\"\n",
        "        )\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 429:\n",
        "                print(f\"Rate limit reached. Retrying in {retry_after} seconds...\")\n",
        "                time.sleep(retry_after)\n",
        "                continue\n",
        "            elif response.status_code == 400:\n",
        "                print(f\"Error fetching page {page}: {response.status_code}, {response.json()}\")\n",
        "                break\n",
        "            elif response.status_code != 200:\n",
        "                print(f\"Error fetching page {page}: {response.status_code}, {response.text}\")\n",
        "                break\n",
        "\n",
        "            data = response.json()\n",
        "            articles = data.get(\"articles\", [])\n",
        "            if articles:\n",
        "                titles.extend(article.get(\"title\", \"No Title\") for article in articles)\n",
        "                if len(articles) < 10:\n",
        "                    print(f\"Last page reached: {page}\")\n",
        "                    break\n",
        "                page += 1\n",
        "            else:\n",
        "                print(f\"No articles on page {page}\")\n",
        "                break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            break\n",
        "\n",
        "    return titles\n",
        "\n",
        "date_ranges = [\n",
        "    (\"2023-01-01\", \"2023-03-31\"),\n",
        "    (\"2023-04-01\", \"2023-06-30\"),\n",
        "    (\"2023-07-01\", \"2023-09-30\"),\n",
        "    (\"2023-10-01\", \"2023-12-31\"),\n",
        "]\n",
        "\n",
        "all_titles = []\n",
        "for start_date, end_date in date_ranges:\n",
        "    print(f\"Fetching articles from {start_date} to {end_date}\")\n",
        "    titles = fetch_titles(from_date=start_date, to_date=end_date,\n",
        "                          max_pages=30, retry_after=5)\n",
        "    all_titles.extend(titles)\n",
        "\n",
        "print(f\"Total fetched titles: {len(all_titles)}\")\n",
        "df = pd.DataFrame({\"Title\": all_titles})\n",
        "\n",
        "def print_sample_titles(df: pd.DataFrame, sample_size: int = 5) -> None:\n",
        "    \"\"\"\n",
        "    Display a sample of the fetched article titles.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame containing a 'Title' column.\n",
        "    sample_size : int, default 5\n",
        "        Number of titles to display.\n",
        "    \"\"\"\n",
        "    print(df.head(sample_size))\n",
        "\n",
        "print_sample_titles(df)\n",
        "\n",
        "def save_titles_to_csv(df: pd.DataFrame, filename: str = \"article_titles.csv\") -> None:\n",
        "    \"\"\"\n",
        "    Save the fetched article titles to a CSV file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame containing a 'Title' column.\n",
        "    filename : str, default \"article_titles.csv\"\n",
        "        Name of the output CSV file.\n",
        "    \"\"\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Titles saved to {filename}\")\n",
        "\n",
        "save_titles_to_csv(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Batch analyze article titles for sensitive topics using the Groq API, then summarize results.\n",
        "\n",
        "- Reads titles from a CSV file.\n",
        "- `process_batch`: identifies the top 10 sensitive topics in each batch of titles.\n",
        "- `summarize_results_batch`: condenses intermediate batch summaries into a final list.\n",
        "- Processes titles in batches and prints the final summary.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from groq import Groq\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"<YOUR_GROQ_API_KEY_HERE>\"\n",
        "\n",
        "client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "\n",
        "# Read titles from CSV\n",
        "csv_file = \"article_titles.csv\"\n",
        "df = pd.read_csv(csv_file)\n",
        "all_titles = df[\"Title\"].tolist()\n",
        "\n",
        "def process_batch(titles_batch):\n",
        "    \"\"\"\n",
        "    Analyze a batch of titles for the top 10 most sensitive topics.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    titles_batch : list of str\n",
        "        A batch of article titles.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str or None\n",
        "        The modelâ€™s ranked list of sensitive topics, or None on error.\n",
        "    \"\"\"\n",
        "    articles_content = \"\\n\".join(f\"Title: {title}\" for title in titles_batch)\n",
        "    prompt = f\"\"\"Analyze the following article titles and identify the top 10 most sensitive\n",
        "topics discussed. Sensitivity includes political, social, economic, or cultural topics that may\n",
        "provoke significant discussion or controversy.\n",
        "\n",
        "Titles:\n",
        "{articles_content}\n",
        "\n",
        "Provide a ranked list of the top 10 sensitive topics.\"\"\"\n",
        "    try:\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            model=\"llama-3.2-90b-vision-preview\"\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing batch: {e}\")\n",
        "        return None\n",
        "\n",
        "def summarize_results_batch(results_batch):\n",
        "    \"\"\"\n",
        "    Summarize multiple batch results into a concise list of sensitive topics.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    results_batch : list of str\n",
        "        Summaries from previous batches.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str or None\n",
        "        The consolidated summary of sensitive topics, or None on error.\n",
        "    \"\"\"\n",
        "    results_content = \"\\n\\n\".join(results_batch)\n",
        "    prompt = f\"\"\"Summarize the following topics from multiple batches into a concise summary of\n",
        "the most sensitive topics.\n",
        "\n",
        "Batch Summaries:\n",
        "{results_content}\n",
        "\n",
        "Provide a summarized list of sensitive topics.\"\"\"\n",
        "    try:\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            model=\"mixtral-8x7b-32768\"\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error summarizing results: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process titles in batches\n",
        "batch_size = 30\n",
        "intermediate_summaries = []\n",
        "for i in range(0, len(all_titles), batch_size):\n",
        "    print(f\"Processing batch {i // batch_size + 1}\")\n",
        "    batch = all_titles[i : i + batch_size]\n",
        "    result = process_batch(batch)\n",
        "    if result:\n",
        "        intermediate_summaries.append(result)\n",
        "\n",
        "# Summarize intermediate results\n",
        "summary_batch_size = 5\n",
        "final_intermediate_summaries = []\n",
        "for i in range(0, len(intermediate_summaries), summary_batch_size):\n",
        "    print(f\"Summarizing intermediate batch {i // summary_batch_size + 1}\")\n",
        "    summary_batch = intermediate_summaries[i : i + summary_batch_size]\n",
        "    summarized_result = summarize_results_batch(summary_batch)\n",
        "    if summarized_result:\n",
        "        final_intermediate_summaries.append(summarized_result)\n",
        "\n",
        "# Final summarization\n",
        "print(\"Final summarization step ...\")\n",
        "final_summary = summarize_results_batch(final_intermediate_summaries)\n",
        "\n",
        "if final_summary:\n",
        "    print(\"Final Analysis:\")\n",
        "    print(final_summary)\n",
        "else:\n",
        "    print(\"No final analysis available.\")\n"
      ],
      "metadata": {
        "id": "ZgYldZf6bpbz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}