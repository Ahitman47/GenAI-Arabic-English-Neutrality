{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pingouin\n",
        "pip install scikit_posthocs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5IFsR4lWh7X",
        "outputId": "92761f07-81b5-4ec1-a81d-c07bf50dc33e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YW3x66h8Ufhq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import scikit_posthocs as sp\n",
        "import pingouin as pg\n",
        "\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import ttest_rel, wilcoxon\n",
        "from scipy.stats import kruskal\n",
        "from scipy.stats import pearsonr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Transform a Qualtrics survey export into a long-form DataFrame.\n",
        "\n",
        "Reads a semicolon-delimited CSV, retains metadata columns, maps each\n",
        "question code to a numeric ID and sensitivity label, renames response\n",
        "suffixes to descriptive names, and concatenates all questions into one\n",
        "tidy DataFrame with a defined column order.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"Survey Results.csv\", delimiter=\";\", encoding=\"utf-8\")\n",
        "\n",
        "meta_cols = [\n",
        "    \"ResponseId\", \"Status\", \"IPAddress\", \"Progress\", \"Finished\",\n",
        "    \"LocationLatitude\", \"LocationLongitude\", \"Duration (in seconds)\"\n",
        "]\n",
        "\n",
        "question_map = {\n",
        "    \"Q10\": (0, \"Non-sensitive\"),\n",
        "    \"Q8\":  (1, \"Non-sensitive\"),\n",
        "    \"Q9\":  (2, \"Non-sensitive\"),\n",
        "    \"Q1\":  (3, \"Sensitive\"),\n",
        "    \"Q3\":  (4, \"Sensitive\"),\n",
        "    \"Q5\":  (5, \"Sensitive\"),\n",
        "    \"Q6\":  (6, \"Sensitive\"),\n",
        "    \"Q2\":  (7, \"Sensitive\"),\n",
        "    \"Q4\":  (8, \"Sensitive\"),\n",
        "    \"Q7\":  (9, \"Sensitive\"),\n",
        "}\n",
        "\n",
        "suffix_map = {\n",
        "    \"-1_1\": \"SentimentConsistency\",\n",
        "    \"-1_2\": \"FactualConsistency\",\n",
        "    \"-2_1\": \"EnglishNeutrality\",\n",
        "    \"-2_2\": \"ArabicNeutrality\",\n",
        "    \"-2_3\": \"QuestionNeutrality\",\n",
        "    \"-3\":   \"Comments\",\n",
        "}\n",
        "\n",
        "rows = []\n",
        "for code, (qid, sensitivity) in question_map.items():\n",
        "    q_cols = [c for c in df.columns if c.startswith(f\"{code}-\")]\n",
        "    if not q_cols:\n",
        "        continue\n",
        "\n",
        "    temp = df[meta_cols + q_cols].copy()\n",
        "    rename_dict = {c: suffix_map[c.replace(code, \"\")] for c in q_cols}\n",
        "    temp.rename(columns=rename_dict, inplace=True)\n",
        "    temp[\"QuestionID\"] = qid\n",
        "    temp[\"Sensitivity\"] = sensitivity\n",
        "    rows.append(temp)\n",
        "\n",
        "long_df = pd.concat(rows, ignore_index=True)\n",
        "\n",
        "final_cols = (\n",
        "    meta_cols\n",
        "    + [\"QuestionID\", \"Sensitivity\"]\n",
        "    + [\n",
        "        \"FactualConsistency\",\n",
        "        \"SentimentConsistency\",\n",
        "        \"EnglishNeutrality\",\n",
        "        \"ArabicNeutrality\",\n",
        "        \"QuestionNeutrality\",\n",
        "    ]\n",
        ")\n",
        "long_df = long_df[final_cols]\n",
        "\n",
        "# print(long_df.head())\n",
        "# print(long_df.shape, long_df.info())\n"
      ],
      "metadata": {
        "id": "BwQNvcZCUjou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Compute the mean survey duration after outlier removal.\n",
        "\n",
        "This script:\n",
        "1. Converts the 'Duration (in seconds)' column to numeric, coercing invalid entries to NaN.\n",
        "2. Identifies outliers using the IQR method and a minimum threshold of 300 seconds.\n",
        "3. Filters out those outliers.\n",
        "4. Calculates and prints the mean duration, counts before and after filtering,\n",
        "   and the IQR bounds used for filtering.\n",
        "\"\"\"\n",
        "\n",
        "df['Duration (in seconds)'] = pd.to_numeric(df['Duration (in seconds)'], errors='coerce')\n",
        "\n",
        "# Extract unique duration values\n",
        "tmp_df = df[['Duration (in seconds)']].drop_duplicates()\n",
        "\n",
        "# Determine IQR bounds\n",
        "Q1 = tmp_df['Duration (in seconds)'].quantile(0.25)\n",
        "Q3 = tmp_df['Duration (in seconds)'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Keep durations ≥ 300 seconds and within IQR bounds\n",
        "tmp_df_no_outliers = tmp_df[\n",
        "    (tmp_df['Duration (in seconds)'] >= 300) &\n",
        "    (tmp_df['Duration (in seconds)'] <= upper_bound)\n",
        "]\n",
        "\n",
        "mean_duration = tmp_df_no_outliers['Duration (in seconds)'].mean()\n",
        "print(f\"Mean Duration (in seconds) after removing outliers = {mean_duration:.2f}\")\n",
        "\n",
        "print(f\"Unique durations before removal: {len(tmp_df)}\")\n",
        "print(f\"Unique durations after removal: {len(tmp_df_no_outliers)}\")\n",
        "print(f\"IQR lower bound: {lower_bound:.2f}, upper bound: {upper_bound:.2f}\")\n"
      ],
      "metadata": {
        "id": "4fOdlCurU-B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Filter and prepare the long-form survey DataFrame for analysis.\n",
        "\n",
        "Steps:\n",
        "1. Convert 'Progress' and 'Duration (in seconds)' to numeric.\n",
        "2. Keep only completed responses with sufficient progress and duration.\n",
        "3. Remove rows missing any key rating metrics.\n",
        "4. Cast metadata and rating columns to appropriate types.\n",
        "5. Report before/after row counts and unique IP–duration combinations, then display the first rows.\n",
        "\"\"\"\n",
        "\n",
        "# Convert columns to numeric types, coercing invalid entries\n",
        "long_df[\"Progress\"] = pd.to_numeric(long_df[\"Progress\"], errors=\"coerce\")\n",
        "long_df[\"Duration (in seconds)\"] = pd.to_numeric(long_df[\"Duration (in seconds)\"], errors=\"coerce\")\n",
        "\n",
        "# Filter for valid, completed responses\n",
        "valid = long_df.loc[\n",
        "    (long_df[\"Status\"] == \"IP Address\") &\n",
        "    (long_df[\"Progress\"] > 60) &\n",
        "    (long_df[\"Duration (in seconds)\"] > 300)\n",
        "]\n",
        "\n",
        "# Drop any row missing one of the five rating metrics\n",
        "metrics = [\n",
        "    \"SentimentConsistency\", \"FactualConsistency\",\n",
        "    \"EnglishNeutrality\", \"ArabicNeutrality\", \"QuestionNeutrality\"\n",
        "]\n",
        "filtered_df = valid.dropna(subset=metrics).copy()\n",
        "\n",
        "# Cast metadata columns to appropriate types\n",
        "filtered_df.loc[:, \"Status\"] = filtered_df[\"Status\"].astype(str)\n",
        "filtered_df.loc[:, \"IPAddress\"] = filtered_df[\"IPAddress\"].astype(str)\n",
        "filtered_df.loc[:, \"Progress\"] = filtered_df[\"Progress\"].astype(int)\n",
        "filtered_df.loc[:, \"Finished\"] = filtered_df[\"Finished\"].astype(bool)\n",
        "filtered_df.loc[:, \"Duration (in seconds)\"] = filtered_df[\"Duration (in seconds)\"].astype(int)\n",
        "filtered_df.loc[:, \"LocationLatitude\"] = filtered_df[\"LocationLatitude\"].astype(str)\n",
        "filtered_df.loc[:, \"LocationLongitude\"] = filtered_df[\"LocationLongitude\"].astype(str)\n",
        "\n",
        "# Convert rating columns to integers\n",
        "for col in metrics:\n",
        "    filtered_df.loc[:, col] = filtered_df[col].astype(int)\n",
        "\n",
        "# Print summary of filtering results\n",
        "print(\"Rows before filtering:\", long_df.shape[0])\n",
        "print(\"Rows after filtering:\", filtered_df.shape[0])\n",
        "unique_pairs = filtered_df[[\"IPAddress\", \"Duration (in seconds)\"]].drop_duplicates().shape[0]\n",
        "print(f\"Unique (IP Address, Duration) combinations: {unique_pairs}\")\n",
        "print(filtered_df.groupby(\"QuestionID\").size())\n",
        "\n",
        "filtered_df.head()\n"
      ],
      "metadata": {
        "id": "KDLJUFBsVMzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Generate overlapping violin plots comparing metric distributions for sensitive\n",
        "and non-sensitive survey responses.\n",
        "\n",
        "Defines a function to plot side-by-side violins for each metric and optionally\n",
        "save the figure. Then demonstrates its use on the filtered DataFrame.\n",
        "\"\"\"\n",
        "\n",
        "def plot_group_violin_overlap(df, metrics, filename=None):\n",
        "    \"\"\"\n",
        "    Create overlapping violin plots for two groups.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        Must contain a 'Sensitivity' column with values \"Non-sensitive\" and \"Sensitive\",\n",
        "        and one column per metric in `metrics`.\n",
        "    metrics : list of str\n",
        "        Column names to plot on the x-axis.\n",
        "    filename : str, optional\n",
        "        If provided, path to save the figure (e.g. \"overlap_violin.png\").\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    positions = range(1, len(metrics) + 1)\n",
        "\n",
        "    # Data for each group\n",
        "    data_non = [df[df[\"Sensitivity\"] == \"Non-sensitive\"][m].dropna().tolist() for m in metrics]\n",
        "    data_sen = [df[df[\"Sensitivity\"] == \"Sensitive\"][m].dropna().tolist() for m in metrics]\n",
        "\n",
        "    vp_non = ax.violinplot(data_non, positions=positions, widths=0.6, showmedians=True)\n",
        "    for part in vp_non['bodies']:\n",
        "        part.set_alpha(0.5)\n",
        "\n",
        "    vp_sen = ax.violinplot(data_sen, positions=positions, widths=0.6, showmedians=True)\n",
        "    for part in vp_sen['bodies']:\n",
        "        part.set_alpha(0.5)\n",
        "\n",
        "    ax.set_xticks(positions)\n",
        "    ax.set_xticklabels(metrics, rotation=45, fontsize=12)\n",
        "    ax.set_ylabel(\"Score (5-point)\", fontsize=12)\n",
        "    ax.set_title(\n",
        "        \"Survey Metric Distributions — Sensitive vs Non-sensitive\",\n",
        "        fontsize=14, fontweight=\"bold\"\n",
        "    )\n",
        "    ax.legend(\n",
        "        [vp_non['bodies'][0], vp_sen['bodies'][0]],\n",
        "        [\"Non-sensitive\", \"Sensitive\"],\n",
        "        loc=\"upper left\"\n",
        "    )\n",
        "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if filename:\n",
        "        fig.savefig(filename, dpi=300)\n",
        "\n",
        "# Example usage:\n",
        "plot_group_violin_overlap(filtered_df, metrics, filename=\"overlap_violin_final.png\")\n"
      ],
      "metadata": {
        "id": "FxPQFsnBVceV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Plot histograms with normal density overlays for each survey metric.\n",
        "\n",
        "For each metric in `metrics`, this script generates two side-by-side plots:\n",
        "- Overall distribution with a fitted normal curve.\n",
        "- Separate distributions for non-sensitive and sensitive responses.\n",
        "\"\"\"\n",
        "\n",
        "for metric in metrics:\n",
        "    fig, axes = plt.subplots(ncols=2, figsize=(10, 4))\n",
        "\n",
        "    # Overall distribution\n",
        "    data_all = filtered_df[metric].dropna().astype(float)\n",
        "    mu, sigma = data_all.mean(), data_all.std()\n",
        "    x = np.linspace(data_all.min(), data_all.max(), 100)\n",
        "    axes[0].hist(data_all, bins=5, density=True, alpha=0.6, edgecolor='black')\n",
        "    axes[0].plot(x, norm.pdf(x, mu, sigma), 'k--')\n",
        "    axes[0].set_title(f\"{metric} — Overall\")\n",
        "    axes[0].set_xlabel(\"Rating\")\n",
        "    axes[0].set_ylabel(\"Density\")\n",
        "\n",
        "    # By sensitivity group\n",
        "    for sens, color in zip([\"Non-sensitive\", \"Sensitive\"], ['#1f77b4', '#ff7f0e']):\n",
        "        group = filtered_df[filtered_df[\"Sensitivity\"] == sens][metric].dropna().astype(float)\n",
        "        mu_g, sigma_g = group.mean(), group.std()\n",
        "        axes[1].hist(group, bins=5, density=True, alpha=0.5, color=color, edgecolor='black')\n",
        "        axes[1].plot(x, norm.pdf(x, mu_g, sigma_g), color=color, linestyle='--')\n",
        "    axes[1].set_title(f\"{metric} — By Sensitivity\")\n",
        "    axes[1].set_xlabel(\"Rating\")\n",
        "    axes[1].legend([\"Non-sensitive\", \"Sensitive\"])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ARkTF3PiVopA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "For each survey metric, plot a 1×3 grid of:\n",
        "- Histogram with fitted normal curve and Q–Q inset for all responses.\n",
        "- The same for non-sensitive responses.\n",
        "- The same for sensitive responses.\n",
        "\"\"\"\n",
        "for metric in metrics:\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "    groups = [\n",
        "        (\"Overall\", filtered_df),\n",
        "        (\"Non-sensitive\", filtered_df[filtered_df[\"Sensitivity\"] == \"Non-sensitive\"]),\n",
        "        (\"Sensitive\", filtered_df[filtered_df[\"Sensitivity\"] == \"Sensitive\"])\n",
        "    ]\n",
        "\n",
        "    for ax, (label, subset) in zip(axes, groups):\n",
        "        data = subset[metric].dropna().astype(float)\n",
        "        mu, sigma = data.mean(), data.std()\n",
        "        x = np.linspace(data.min(), data.max(), 100)\n",
        "\n",
        "        ax.hist(data, bins=5, density=True, alpha=0.6, edgecolor=\"black\")\n",
        "        ax.plot(x, st.norm.pdf(x, mu, sigma), 'k--')\n",
        "\n",
        "        qq = ax.inset_axes([0.6, 0.6, 0.35, 0.35])\n",
        "        st.probplot(data, dist=\"norm\", plot=qq)\n",
        "        qq.set_title(\"Q–Q\", fontsize=8)\n",
        "\n",
        "        ax.set_title(f\"{metric} — {label}\")\n",
        "        ax.set_xlabel(\"Rating\")\n",
        "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "5aUaIV2ZWAVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Compute paired statistical tests and effect sizes for survey metrics.\n",
        "\n",
        "Aggregates each respondent’s average ratings by sensitivity, then for each\n",
        "metric performs paired t-tests, calculates Cohen’s d, and runs Wilcoxon tests\n",
        "where applicable. Results are collected into a summary DataFrame.\n",
        "\"\"\"\n",
        "\n",
        "# Aggregate average ratings per respondent and sensitivity\n",
        "agg = (\n",
        "    filtered_df\n",
        "    .groupby([\"ResponseId\", \"Sensitivity\"])[metrics]\n",
        "    .mean()\n",
        "    .unstack(\"Sensitivity\")\n",
        ")\n",
        "\n",
        "results = []\n",
        "for metric in metrics:\n",
        "    # Extract paired non-sensitive and sensitive values\n",
        "    non_vals = agg[(metric, \"Non-sensitive\")].dropna().astype(float).values\n",
        "    sen_vals = agg[(metric, \"Sensitive\")].dropna().astype(float).values\n",
        "    paired = pd.DataFrame({\"non\": non_vals, \"sen\": sen_vals}).dropna()\n",
        "    non_vals = paired[\"non\"].values\n",
        "    sen_vals = paired[\"sen\"].values\n",
        "\n",
        "    # Paired t-test and Cohen’s d\n",
        "    t_stat, t_p = ttest_rel(sen_vals, non_vals)\n",
        "    cohens_d = (sen_vals - non_vals).mean() / (sen_vals - non_vals).std(ddof=1)\n",
        "\n",
        "    # Wilcoxon signed-rank test if sufficient variation and sample size\n",
        "    if len(paired) >= 10 and not np.allclose(sen_vals, non_vals):\n",
        "        w_stat, w_p = wilcoxon(sen_vals, non_vals)\n",
        "    else:\n",
        "        w_stat, w_p = float(\"nan\"), float(\"nan\")\n",
        "\n",
        "    results.append({\n",
        "        \"Metric\":      metric,\n",
        "        \"Paired t\":    round(t_stat, 3),\n",
        "        \"p (t-test)\":  round(t_p, 4),\n",
        "        \"Cohen’s d\":   round(cohens_d, 3),\n",
        "        \"Wilcoxon W\":  w_stat,\n",
        "        \"p (Wilcoxon)\": round(w_p, 4)\n",
        "    })\n",
        "\n",
        "# Create and display the comparison table\n",
        "comparison_df = pd.DataFrame(results).set_index(\"Metric\")\n",
        "print(comparison_df)\n"
      ],
      "metadata": {
        "id": "laeDavKzWN5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Compute Kruskal–Wallis tests across questions for each metric within each sensitivity group.\n",
        "\n",
        "For each sensitivity level and metric, this script:\n",
        "1. Collects ratings by QuestionID.\n",
        "2. Runs a Kruskal–Wallis test if there are at least two non-empty groups.\n",
        "3. Aggregates H statistics and p-values into a DataFrame indexed by (Sensitivity, Metric).\n",
        "\"\"\"\n",
        "\n",
        "results = []\n",
        "for sens in filtered_df[\"Sensitivity\"].unique():\n",
        "    for metric in metrics:\n",
        "        groups = [\n",
        "            filtered_df[\n",
        "                (filtered_df[\"Sensitivity\"] == sens) &\n",
        "                (filtered_df[\"QuestionID\"] == qid)\n",
        "            ][metric].dropna().astype(float)\n",
        "            for qid in filtered_df[\"QuestionID\"].unique()\n",
        "        ]\n",
        "        groups = [g for g in groups if len(g) > 0]\n",
        "        H, p = kruskal(*groups) if len(groups) > 1 else (None, None)\n",
        "        results.append({\n",
        "            \"Sensitivity\": sens,\n",
        "            \"Metric\": metric,\n",
        "            \"H\": round(H, 3) if H is not None else None,\n",
        "            \"p-value\": round(p, 4) if p is not None else None\n",
        "        })\n",
        "\n",
        "question_level_df = pd.DataFrame(results).set_index([\"Sensitivity\", \"Metric\"])\n",
        "print(question_level_df)\n"
      ],
      "metadata": {
        "id": "JPFk1_JaWQu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Plot mean survey ratings by question for each metric using grouped bars.\n",
        "\n",
        "This script:\n",
        "1. Calculates the mean of each metric grouped by QuestionID.\n",
        "2. Displays a grouped bar chart where each question’s mean ratings are side-by-side\n",
        "   for easy comparison across metrics.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mean_table = filtered_df.groupby(\"QuestionID\")[metrics].mean().astype(float)\n",
        "n_questions = mean_table.shape[0]\n",
        "bar_width = 0.8 / n_questions\n",
        "x = np.arange(len(metrics))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "\n",
        "for i, qid in enumerate(mean_table.index):\n",
        "    ax.bar(\n",
        "        x + i * bar_width,\n",
        "        mean_table.loc[qid],\n",
        "        width=bar_width,\n",
        "        alpha=0.6,\n",
        "        label=f\"Q{qid}\"\n",
        "    )\n",
        "\n",
        "ax.set_xticks(x + bar_width * (n_questions - 1) / 2)\n",
        "ax.set_xticklabels(metrics, rotation=45, fontsize=12)\n",
        "ax.set_ylabel(\"Mean rating (5-point)\", fontsize=12)\n",
        "ax.set_title(\"Mean Survey Ratings by Question within Each Metric\",\n",
        "             fontsize=14, fontweight=\"bold\")\n",
        "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
        "ax.legend(title=\"Question ID\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", borderaxespad=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "41I1js2OW8Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Perform post-hoc Dunn’s tests for pairwise comparisons of question ratings.\n",
        "\n",
        "For each sensitivity group (\"Non-sensitive\" and \"Sensitive\") and each metric,\n",
        "runs Dunn’s test with Bonferroni-adjusted p-values across QuestionID groups,\n",
        "and prints the resulting significance matrices.\n",
        "\"\"\"\n",
        "\n",
        "for sens in filtered_df[\"Sensitivity\"].unique():\n",
        "    print(f\"\\nPost-hoc Dunn’s test — {sens}\")\n",
        "    for metric in metrics:\n",
        "        df_sub = (\n",
        "            filtered_df[filtered_df[\"Sensitivity\"] == sens]\n",
        "            [[\"QuestionID\", metric]]\n",
        "            .dropna()\n",
        "        )\n",
        "        if df_sub[\"QuestionID\"].nunique() > 1:\n",
        "            dunn = sp.posthoc_dunn(\n",
        "                df_sub,\n",
        "                val_col=metric,\n",
        "                group_col=\"QuestionID\",\n",
        "                p_adjust=\"bonferroni\"\n",
        "            )\n",
        "            print(f\"\\n{metric}:\\n{dunn.round(4)}\")\n"
      ],
      "metadata": {
        "id": "h0zRkdS-W87x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Compute Pearson correlation matrices and plot heatmaps for survey metrics.\n",
        "\n",
        "Defines:\n",
        "- compute_corr_and_pvals(df, metrics): returns corr coefficients and p-values for given metrics.\n",
        "- plot_corr_heatmap(corr, pvals, title, metrics): visualizes a correlation matrix with significance stars.\n",
        "\n",
        "Then runs both overall and per-sensitivity analyses on `filtered_df` for the specified `metrics`.\n",
        "\"\"\"\n",
        "\n",
        "def compute_corr_and_pvals(df, metrics):\n",
        "    corr = pd.DataFrame(index=metrics, columns=metrics, dtype=float)\n",
        "    pvals = pd.DataFrame(index=metrics, columns=metrics, dtype=float)\n",
        "    for i in metrics:\n",
        "        for j in metrics:\n",
        "            x = df[i].astype(float)\n",
        "            y = df[j].astype(float)\n",
        "            r, p = pearsonr(x, y)\n",
        "            corr.loc[i, j] = round(r, 3)\n",
        "            pvals.loc[i, j] = round(p, 4)\n",
        "    return corr, pvals\n",
        "\n",
        "def plot_corr_heatmap(corr, pvals, title, metrics):\n",
        "    mask = np.eye(len(metrics), dtype=bool)\n",
        "    fig, ax = plt.subplots(figsize=(7, 6))\n",
        "    cmap = plt.cm.coolwarm\n",
        "    im = ax.imshow(np.ma.masked_where(mask, corr), cmap=cmap, vmin=-1, vmax=1)\n",
        "\n",
        "    # Annotate cells and add a star (★) if p-value < 0.05\n",
        "    for i in range(len(metrics)):\n",
        "        for j in range(len(metrics)):\n",
        "            if not mask[i, j]:\n",
        "                val = corr.iloc[i, j]\n",
        "                text = f\"{val:.2f}\"\n",
        "                if pvals.iloc[i, j] < 0.05:\n",
        "                    text += \"★\"\n",
        "                color = \"white\" if abs(val) > 0.3 else \"black\"\n",
        "                ax.text(j, i, text, ha=\"center\", va=\"center\", color=color, fontsize=11)\n",
        "\n",
        "    ax.set_xticks(np.arange(len(metrics)))\n",
        "    ax.set_yticks(np.arange(len(metrics)))\n",
        "    ax.set_xticklabels(metrics, rotation=45, ha=\"right\", fontsize=12)\n",
        "    ax.set_yticklabels(metrics, fontsize=12)\n",
        "    ax.set_title(title, fontsize=16, pad=20)\n",
        "    cbar = fig.colorbar(im, ax=ax, shrink=0.8, pad=0.02)\n",
        "    cbar.set_label(\"Pearson r\", fontsize=12)\n",
        "    ax.grid(False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "corr_overall, pvals_overall = compute_corr_and_pvals(filtered_df, metrics)\n",
        "print(\"Overall Correlation Matrix (r):\\n\", corr_overall)\n",
        "print(\"\\nOverall P-values:\\n\", pvals_overall)\n",
        "plot_corr_heatmap(\n",
        "    corr_overall,\n",
        "    pvals_overall,\n",
        "    \"Overall Correlation Matrix of Survey Metrics ★ = p<0.05\",\n",
        "    metrics\n",
        ")\n",
        "\n",
        "for sens in filtered_df[\"Sensitivity\"].unique():\n",
        "    sens_df = filtered_df[filtered_df[\"Sensitivity\"] == sens]\n",
        "    corr_sens, pvals_sens = compute_corr_and_pvals(sens_df, metrics)\n",
        "    print(f\"\\nCorrelation Matrix for: {sens} Questions (r):\\n\", corr_sens)\n",
        "    print(f\"\\nP-values for Sensitivity = {sens}:\\n\", pvals_sens)\n",
        "    plot_corr_heatmap(\n",
        "        corr_sens,\n",
        "        pvals_sens,\n",
        "        f\"Correlation Matrix for: {sens} Questions ★ = p<0.05\",\n",
        "        metrics\n",
        "    )\n"
      ],
      "metadata": {
        "id": "2HKREVhqXNks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Run paired Wilcoxon tests for neutrality and consistency metrics.\n",
        "\n",
        "Compares ArabicNeutrality vs. EnglishNeutrality and SentimentConsistency vs. FactualConsistency\n",
        "across three groups: overall, sensitive, and non-sensitive. Records test statistic W, p-value,\n",
        "and rank-biserial correlation r for each comparison.\n",
        "\"\"\"\n",
        "\n",
        "comparisons = [\n",
        "    (\"ArabicNeutrality\", \"EnglishNeutrality\", \"Neutrality\"),\n",
        "    (\"SentimentConsistency\", \"FactualConsistency\", \"Consistency\")\n",
        "]\n",
        "groups = [\n",
        "    (\"Overall\", filtered_df),\n",
        "    (\"Sensitive\", filtered_df[filtered_df[\"Sensitivity\"] == \"Sensitive\"]),\n",
        "    (\"Non-sensitive\", filtered_df[filtered_df[\"Sensitivity\"] == \"Non-sensitive\"])\n",
        "]\n",
        "\n",
        "results = []\n",
        "for col1, col2, label in comparisons:\n",
        "    for group_name, df_sub in groups:\n",
        "        df_pair = df_sub[[col1, col2]].apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
        "        if len(df_pair) >= 10:\n",
        "            res = pg.wilcoxon(df_pair[col1], df_pair[col2])\n",
        "            results.append({\n",
        "                \"Comparison\": label,\n",
        "                \"Group\": group_name,\n",
        "                \"W\": round(res[\"W-val\"].iloc[0], 3),\n",
        "                \"p-value\": round(res[\"p-val\"].iloc[0], 4),\n",
        "                \"r\": round(res[\"RBC\"].iloc[0], 3)\n",
        "            })\n",
        "        else:\n",
        "            results.append({\n",
        "                \"Comparison\": label,\n",
        "                \"Group\": group_name,\n",
        "                \"W\": None,\n",
        "                \"p-value\": None,\n",
        "                \"r\": None\n",
        "            })\n",
        "\n",
        "comparison_df = pd.DataFrame(results).set_index([\"Comparison\", \"Group\"])\n",
        "print(comparison_df)"
      ],
      "metadata": {
        "id": "1OKqw3fNXOXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Perform paired t-tests for neutrality and consistency metrics across groups.\n",
        "\n",
        "Compares:\n",
        "- ArabicNeutrality vs. EnglishNeutrality\n",
        "- SentimentConsistency vs. FactualConsistency\n",
        "\n",
        "for each of:\n",
        "- Overall responses\n",
        "- Sensitive responses\n",
        "- Non-sensitive responses\n",
        "\n",
        "Outputs a table with T-statistics, p-values, and Cohen’s d, indexed by Comparison and Group.\n",
        "\"\"\"\n",
        "\n",
        "comparisons = [\n",
        "    (\"ArabicNeutrality\", \"EnglishNeutrality\", \"Neutrality\"),\n",
        "    (\"SentimentConsistency\", \"FactualConsistency\", \"Consistency\")\n",
        "]\n",
        "groups = [\n",
        "    (\"Overall\", filtered_df),\n",
        "    (\"Sensitive\", filtered_df[filtered_df[\"Sensitivity\"] == \"Sensitive\"]),\n",
        "    (\"Non-sensitive\", filtered_df[filtered_df[\"Sensitivity\"] == \"Non-sensitive\"])\n",
        "]\n",
        "\n",
        "results = []\n",
        "for col1, col2, label in comparisons:\n",
        "    for group_name, df_sub in groups:\n",
        "        df_pair = df_sub[[col1, col2]].apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
        "        if len(df_pair) >= 10:\n",
        "            res = pg.ttest(df_pair[col1], df_pair[col2], paired=True)\n",
        "            results.append({\n",
        "                \"Comparison\": label,\n",
        "                \"Group\": group_name,\n",
        "                \"T\": round(res[\"T\"].iloc[0], 3),\n",
        "                \"p-value\": round(res[\"p-val\"].iloc[0], 4),\n",
        "                \"Cohen's d\": round(res[\"cohen-d\"].iloc[0], 3)\n",
        "            })\n",
        "        else:\n",
        "            results.append({\n",
        "                \"Comparison\": label,\n",
        "                \"Group\": group_name,\n",
        "                \"T\": None,\n",
        "                \"p-value\": None,\n",
        "                \"Cohen's d\": None\n",
        "            })\n",
        "\n",
        "comparison_df = pd.DataFrame(results).set_index([\"Comparison\", \"Group\"])\n",
        "print(comparison_df)\n"
      ],
      "metadata": {
        "id": "TrosICESYhPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Compute and visualize mean differences between survey metrics.\n",
        "\n",
        "This module provides:\n",
        "- compute_diff_means_and_pvals: calculate mean differences and p-values (paired t-test)\n",
        "  between each pair of metrics.\n",
        "- plot_diff_heatmap: display a heatmap of these mean differences with significance stars.\n",
        "\n",
        "It then runs an overall analysis and repeats per sensitivity group.\n",
        "\"\"\"\n",
        "\n",
        "def compute_diff_means_and_pvals(df, metrics):\n",
        "    \"\"\"\n",
        "    Compute mean difference and p-value matrices between metrics.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame containing the metric columns.\n",
        "    metrics : list of str\n",
        "        Metric column names to compare.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    diff_matrix : pandas.DataFrame\n",
        "        Mean differences for each metric pair.\n",
        "    pvals : pandas.DataFrame\n",
        "        Paired t-test p-values for each metric pair.\n",
        "    \"\"\"\n",
        "    diff_matrix = pd.DataFrame(index=metrics, columns=metrics, dtype=float)\n",
        "    pvals = pd.DataFrame(index=metrics, columns=metrics, dtype=float)\n",
        "    for i in metrics:\n",
        "        for j in metrics:\n",
        "            if i == j:\n",
        "                diff_matrix.loc[i, j] = np.nan\n",
        "                pvals.loc[i, j] = np.nan\n",
        "            else:\n",
        "                x = df[i].astype(float)\n",
        "                y = df[j].astype(float)\n",
        "                diff = np.mean(x - y)\n",
        "                t, p = ttest_rel(x, y)\n",
        "                diff_matrix.loc[i, j] = round(diff, 3)\n",
        "                pvals.loc[i, j] = round(p, 4)\n",
        "    return diff_matrix, pvals\n",
        "\n",
        "def plot_diff_heatmap(diff_matrix, pvals, title, metrics):\n",
        "    \"\"\"\n",
        "    Plot a heatmap of mean differences with significance annotations.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    diff_matrix : pandas.DataFrame\n",
        "        Matrix of mean differences between metrics.\n",
        "    pvals : pandas.DataFrame\n",
        "        Corresponding matrix of p-values.\n",
        "    title : str\n",
        "        Title for the heatmap.\n",
        "    metrics : list of str\n",
        "        Order of metric names for axes.\n",
        "    \"\"\"\n",
        "    mask = np.eye(len(metrics), dtype=bool)\n",
        "    fig, ax = plt.subplots(figsize=(7, 6))\n",
        "    max_abs = np.nanmax(np.abs(diff_matrix.values))\n",
        "    im = ax.imshow(\n",
        "        np.ma.masked_where(mask, diff_matrix),\n",
        "        cmap=plt.cm.coolwarm,\n",
        "        vmin=-max_abs,\n",
        "        vmax=max_abs\n",
        "    )\n",
        "\n",
        "    for i in range(len(metrics)):\n",
        "        for j in range(len(metrics)):\n",
        "            if not mask[i, j]:\n",
        "                val = diff_matrix.iloc[i, j]\n",
        "                text = f\"{val:.2f}\"\n",
        "                if pvals.iloc[i, j] < 0.05:\n",
        "                    text += \"★\"\n",
        "                color = \"white\" if abs(val) > (0.3 * max_abs) else \"black\"\n",
        "                ax.text(j, i, text, ha=\"center\", va=\"center\", color=color, fontsize=11)\n",
        "\n",
        "    ax.set_xticks(np.arange(len(metrics)))\n",
        "    ax.set_yticks(np.arange(len(metrics)))\n",
        "    ax.set_xticklabels(metrics, rotation=45, ha=\"right\", fontsize=12)\n",
        "    ax.set_yticklabels(metrics, fontsize=12)\n",
        "    ax.set_title(title, fontsize=16, pad=20)\n",
        "    cbar = fig.colorbar(im, ax=ax, shrink=0.8, pad=0.02)\n",
        "    cbar.set_label(\"Mean Difference\", fontsize=12)\n",
        "    ax.grid(False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Overall mean difference analysis\n",
        "diff_overall, pvals_overall = compute_diff_means_and_pvals(filtered_df, metrics)\n",
        "print(\"Overall Mean Difference Matrix:\\n\", diff_overall)\n",
        "print(\"\\nOverall P-values:\\n\", pvals_overall)\n",
        "plot_diff_heatmap(\n",
        "    diff_overall,\n",
        "    pvals_overall,\n",
        "    \"Overall Mean Difference between Metrics ★ = p<0.05\",\n",
        "    metrics\n",
        ")\n",
        "\n",
        "# Per-sensitivity analysis\n",
        "for sens in filtered_df[\"Sensitivity\"].unique():\n",
        "    sens_df = filtered_df[filtered_df[\"Sensitivity\"] == sens]\n",
        "    diff_sens, pvals_sens = compute_diff_means_and_pvals(sens_df, metrics)\n",
        "    print(f\"\\nMean Difference Matrix for Sensitivity = {sens}:\\n\", diff_sens)\n",
        "    print(f\"\\nP-values for Sensitivity = {sens}:\\n\", pvals_sens)\n",
        "    plot_diff_heatmap(\n",
        "        diff_sens,\n",
        "        pvals_sens,\n",
        "        f\"Mean Difference between Metrics for Sensitivity: {sens} ★ = p<0.05\",\n",
        "        metrics\n",
        "    )\n"
      ],
      "metadata": {
        "id": "7wwHhcsAYuUl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}