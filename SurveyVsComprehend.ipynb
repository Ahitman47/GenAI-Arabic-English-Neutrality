{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AS7gY0f0Y9UA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr, spearmanr, wilcoxon, kendalltau\n",
        "\n",
        "%matplotlib inline\n",
        "pd.set_option('display.precision', 3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load and standardize automated sentiment analysis results.\n",
        "\n",
        "- Reads sentiment analysis Excel file.\n",
        "- Renames columns for clarity.\n",
        "- Adds Sensitivity labels based on QuestionID.\n",
        "- Converts neutrality scores from [0,1] → [1,5].\n",
        "- Normalizes selected metrics to [0,1], preserving originals in *_old columns.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "sentiment_file = \"sentiment_analysis_full_results raw.xlsx\"\n",
        "df_auto = pd.read_excel(sentiment_file)\n",
        "\n",
        "df_auto = df_auto.rename(columns={\n",
        "    \"question_id\": \"QuestionID\",\n",
        "    \"sentiment_consistency_between_arabic_and_english\": \"AutoConsistency\",\n",
        "    \"model\": \"Model\"\n",
        "})\n",
        "\n",
        "df_auto[\"Sensitivity\"] = df_auto[\"QuestionID\"].apply(\n",
        "    lambda x: \"Non-sensitive\" if x in [0, 1, 2] else \"Sensitive\"\n",
        ")\n",
        "\n",
        "if \"NeutralScore_ar\" in df_auto.columns:\n",
        "    df_auto[\"NeutralScore_ar_std\"] = 1 + 4 * df_auto[\"NeutralScore_ar\"]\n",
        "if \"NeutralScore_en\" in df_auto.columns:\n",
        "    df_auto[\"NeutralScore_en_std\"] = 1 + 4 * df_auto[\"NeutralScore_en\"]\n",
        "\n",
        "for col in [\"AutoConsistency\", \"NeutralScore_ar\", \"NeutralScore_en\"]:\n",
        "    if col in df_auto.columns:\n",
        "        df_auto[col + \"_old\"] = df_auto[col]\n",
        "        df_auto[col] = (\n",
        "            df_auto[col] - df_auto[col].min()\n",
        "        ) / (df_auto[col].max() - df_auto[col].min())\n",
        "\n",
        "print(\"Automated analysis data (with standardized metrics):\")\n",
        "display(df_auto)\n"
      ],
      "metadata": {
        "id": "TEfvAlIuZDTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load, reshape, filter, and standardize Qualtrics survey responses.\n",
        "\n",
        "- Reads a semicolon-delimited CSV of raw survey data.\n",
        "- Retains metadata columns unchanged.\n",
        "- Maps question codes to numeric IDs and sensitivity labels.\n",
        "- Renames response columns to descriptive metric names.\n",
        "- Concatenates into a long-form DataFrame.\n",
        "- Converts progress and duration to numeric, filters for completed responses.\n",
        "- Drops rows missing any key metrics and casts types.\n",
        "- Standardizes each survey metric to a 0–1 scale, preserving originals.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Path to your survey CSV\n",
        "survey_file = \"Survey Results.csv\"\n",
        "\n",
        "df_survey = pd.read_csv(survey_file, delimiter=\";\", encoding=\"utf-8\")\n",
        "\n",
        "meta_cols = [\n",
        "    \"ResponseId\", \"Status\", \"IPAddress\", \"Progress\",\n",
        "    \"Finished\", \"LocationLatitude\", \"LocationLongitude\",\n",
        "    \"Duration (in seconds)\"\n",
        "]\n",
        "\n",
        "# Map from Qualtrics question codes → (question ID, sensitivity)\n",
        "question_map = {\n",
        "    \"Q10\": (0, \"Non-sensitive\"),\n",
        "    \"Q8\":  (1, \"Non-sensitive\"),\n",
        "    \"Q9\":  (2, \"Non-sensitive\"),\n",
        "    \"Q1\":  (3, \"Sensitive\"),\n",
        "    \"Q3\":  (4, \"Sensitive\"),\n",
        "    \"Q5\":  (5, \"Sensitive\"),\n",
        "    \"Q6\":  (6, \"Sensitive\"),\n",
        "    \"Q2\":  (7, \"Sensitive\"),\n",
        "    \"Q4\":  (8, \"Sensitive\"),\n",
        "    \"Q7\":  (9, \"Sensitive\"),\n",
        "}\n",
        "\n",
        "# Map from Qualtrics suffix → column name\n",
        "suffix_map = {\n",
        "    \"-1_1\": \"SentimentConsistency\",\n",
        "    \"-1_2\": \"FactualConsistency\",\n",
        "    \"-2_1\": \"EnglishNeutrality\",\n",
        "    \"-2_2\": \"ArabicNeutrality\",\n",
        "    \"-2_3\": \"QuestionNeutrality\",\n",
        "    \"-3\":   \"Comments\",\n",
        "}\n",
        "\n",
        "rows = []\n",
        "for q_code, (qid, sensitivity) in question_map.items():\n",
        "    q_cols = [c for c in df_survey.columns if c.startswith(q_code + \"-\")]\n",
        "    if not q_cols:\n",
        "        continue\n",
        "    tmp = df_survey[meta_cols + q_cols].copy()\n",
        "    rename_dict = {\n",
        "        c: suffix_map[c.replace(q_code, \"\")]\n",
        "        for c in q_cols\n",
        "        if c.replace(q_code, \"\") in suffix_map\n",
        "    }\n",
        "    tmp = tmp.rename(columns=rename_dict)\n",
        "    tmp[\"QuestionID\"] = qid\n",
        "    tmp[\"Sensitivity\"] = sensitivity\n",
        "    rows.append(tmp)\n",
        "\n",
        "# Combine into a long DataFrame\n",
        "long_df = pd.concat(rows, ignore_index=True)\n",
        "\n",
        "final_cols = meta_cols + [\n",
        "    \"QuestionID\", \"Sensitivity\",\n",
        "    \"FactualConsistency\", \"SentimentConsistency\",\n",
        "    \"EnglishNeutrality\", \"ArabicNeutrality\", \"QuestionNeutrality\"\n",
        "]\n",
        "long_df = long_df[final_cols]\n",
        "\n",
        "# Convert numeric fields\n",
        "long_df[\"Progress\"] = pd.to_numeric(long_df[\"Progress\"], errors=\"coerce\")\n",
        "long_df[\"Duration (in seconds)\"] = pd.to_numeric(long_df[\"Duration (in seconds)\"], errors=\"coerce\")\n",
        "\n",
        "# Filter out partial/incomplete responses\n",
        "metrics = [\n",
        "    \"SentimentConsistency\", \"FactualConsistency\",\n",
        "    \"EnglishNeutrality\", \"ArabicNeutrality\", \"QuestionNeutrality\"\n",
        "]\n",
        "valid_df = (\n",
        "    long_df.loc[\n",
        "        (long_df[\"Status\"] == \"IP Address\") &\n",
        "        (long_df[\"Progress\"] > 60) &\n",
        "        (long_df[\"Duration (in seconds)\"] > 300)\n",
        "    ]\n",
        "    .dropna(subset=metrics)\n",
        "    .copy()\n",
        ")\n",
        "\n",
        "# Cast metrics to integers\n",
        "for col in metrics:\n",
        "    valid_df[col] = valid_df[col].astype(int)\n",
        "\n",
        "# Standardize survey metrics to a 0–1 scale, preserving originals\n",
        "for col in metrics:\n",
        "    valid_df[col + \"_old\"] = valid_df[col]\n",
        "    valid_df[col] = (\n",
        "        valid_df[col] - valid_df[col].min()\n",
        "    ) / (valid_df[col].max() - valid_df[col].min())\n",
        "\n",
        "print(\"Survey data (each row is one participant response, with standardized metrics):\")\n",
        "display(valid_df)\n"
      ],
      "metadata": {
        "id": "oRRD1oE2ZPGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Aggregate automated and survey consistency metrics by question,\n",
        "generate ranking tables, and compute Spearman's rank correlations.\n",
        "\n",
        "Steps:\n",
        "1. Average AutoConsistency by QuestionID → df_auto_agg.\n",
        "2. Average SurveyConsistency by QuestionID → df_survey_agg.\n",
        "3. Create overall, non-sensitive, and sensitive ranking tables.\n",
        "4. Compute Spearman correlations for each grouping.\n",
        "\"\"\"\n",
        "\n",
        "df_auto_agg = df_auto.groupby(\"QuestionID\", as_index=False).agg({\n",
        "    \"AutoConsistency\": \"mean\",\n",
        "    \"Sensitivity\": \"first\"\n",
        "})\n",
        "\n",
        "print(\"Aggregated Automated Data:\")\n",
        "display(df_auto_agg)\n",
        "\n",
        "df_survey_agg = valid_df.groupby(\"QuestionID\", as_index=False).agg({\n",
        "    \"SentimentConsistency\": \"mean\",\n",
        "    \"Sensitivity\": \"first\"\n",
        "}).rename(columns={\"SentimentConsistency\": \"SurveyConsistency\"})\n",
        "\n",
        "print(\"Aggregated Survey Data:\")\n",
        "display(df_survey_agg)\n",
        "\n",
        "survey_rank = df_survey_agg.sort_values(\"SurveyConsistency\", ascending=False).reset_index(drop=True)\n",
        "survey_rank[\"Rank\"] = survey_rank.index + 1\n",
        "survey_rank = survey_rank.rename(columns={\"QuestionID\": \"SurveyQuestionID\"})\n",
        "\n",
        "auto_rank = df_auto_agg.sort_values(\"AutoConsistency\", ascending=False).reset_index(drop=True)\n",
        "auto_rank[\"Rank\"] = auto_rank.index + 1\n",
        "auto_rank = auto_rank.rename(columns={\"QuestionID\": \"AutoQuestionID\"})\n",
        "\n",
        "overall_ranking = pd.merge(\n",
        "    survey_rank[[\"Rank\", \"SurveyQuestionID\", \"SurveyConsistency\", \"Sensitivity\"]],\n",
        "    auto_rank[[\"Rank\", \"AutoQuestionID\", \"AutoConsistency\"]],\n",
        "    on=\"Rank\"\n",
        ")\n",
        "\n",
        "print(\"Overall Ranking Table (Rank 1 = most consistent, Rank 10 = least consistent):\")\n",
        "display(overall_ranking)\n",
        "\n",
        "overall_rho, overall_p = spearmanr(\n",
        "    df_survey_agg[\"SurveyConsistency\"],\n",
        "    df_auto_agg[\"AutoConsistency\"]\n",
        ")\n",
        "print(f\"Overall Spearman's Rank Correlation: rho = {overall_rho:.3f}, p = {overall_p:.4g}\")\n",
        "\n",
        "df_survey_non = df_survey_agg[df_survey_agg[\"Sensitivity\"] == \"Non-sensitive\"].copy()\n",
        "df_auto_non   = df_auto_agg[df_auto_agg[\"Sensitivity\"] == \"Non-sensitive\"].copy()\n",
        "\n",
        "survey_rank_non = df_survey_non.sort_values(\"SurveyConsistency\", ascending=False).reset_index(drop=True)\n",
        "survey_rank_non[\"Rank\"] = survey_rank_non.index + 1\n",
        "survey_rank_non = survey_rank_non.rename(columns={\"QuestionID\": \"SurveyQuestionID\"})\n",
        "\n",
        "auto_rank_non = df_auto_non.sort_values(\"AutoConsistency\", ascending=False).reset_index(drop=True)\n",
        "auto_rank_non[\"Rank\"] = auto_rank_non.index + 1\n",
        "auto_rank_non = auto_rank_non.rename(columns={\"QuestionID\": \"AutoQuestionID\"})\n",
        "\n",
        "nonsensitive_ranking = pd.merge(\n",
        "    survey_rank_non[[\"Rank\", \"SurveyQuestionID\", \"SurveyConsistency\", \"Sensitivity\"]],\n",
        "    auto_rank_non[[\"Rank\", \"AutoQuestionID\", \"AutoConsistency\"]],\n",
        "    on=\"Rank\"\n",
        ")\n",
        "\n",
        "print(\"\\nNon-sensitive Ranking Table:\")\n",
        "display(nonsensitive_ranking)\n",
        "\n",
        "if len(df_survey_non) > 1 and len(df_auto_non) > 1:\n",
        "    nonsens_rho, nonsens_p = spearmanr(\n",
        "        df_survey_non[\"SurveyConsistency\"],\n",
        "        df_auto_non[\"AutoConsistency\"]\n",
        "    )\n",
        "    print(f\"Non-sensitive Spearman's Rank Correlation: rho = {nonsens_rho:.3f}, p = {nonsens_p:.4g}\")\n",
        "else:\n",
        "    print(\"Not enough non-sensitive data for Spearman correlation.\")\n",
        "\n",
        "df_survey_sens = df_survey_agg[df_survey_agg[\"Sensitivity\"] == \"Sensitive\"].copy()\n",
        "df_auto_sens   = df_auto_agg[df_auto_agg[\"Sensitivity\"] == \"Sensitive\"].copy()\n",
        "\n",
        "survey_rank_sens = df_survey_sens.sort_values(\"SurveyConsistency\", ascending=False).reset_index(drop=True)\n",
        "survey_rank_sens[\"Rank\"] = survey_rank_sens.index + 1\n",
        "survey_rank_sens = survey_rank_sens.rename(columns={\"QuestionID\": \"SurveyQuestionID\"})\n",
        "\n",
        "auto_rank_sens = df_auto_sens.sort_values(\"AutoConsistency\", ascending=False).reset_index(drop=True)\n",
        "auto_rank_sens[\"Rank\"] = auto_rank_sens.index + 1\n",
        "auto_rank_sens = auto_rank_sens.rename(columns={\"QuestionID\": \"AutoQuestionID\"})\n",
        "\n",
        "sensitive_ranking = pd.merge(\n",
        "    survey_rank_sens[[\"Rank\", \"SurveyQuestionID\", \"SurveyConsistency\", \"Sensitivity\"]],\n",
        "    auto_rank_sens[[\"Rank\", \"AutoQuestionID\", \"AutoConsistency\"]],\n",
        "    on=\"Rank\"\n",
        ")\n",
        "\n",
        "print(\"\\nSensitive Ranking Table:\")\n",
        "display(sensitive_ranking)\n",
        "\n",
        "if len(df_survey_sens) > 1 and len(df_auto_sens) > 1:\n",
        "    sens_rho, sens_p = spearmanr(\n",
        "        df_survey_sens[\"SurveyConsistency\"],\n",
        "        df_auto_sens[\"AutoConsistency\"]\n",
        "    )\n",
        "    print(f\"Sensitive Spearman's Rank Correlation: rho = {sens_rho:.3f}, p = {sens_p:.4g}\")\n",
        "else:\n",
        "    print(\"Not enough sensitive data for Spearman correlation.\")\n"
      ],
      "metadata": {
        "id": "7GcognDgZkOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Compute Pearson correlations between survey and automated standardized metrics.\n",
        "\n",
        "- Aggregate survey data by QuestionID, averaging standardized survey metrics.\n",
        "- Aggregate automated data by QuestionID, averaging standardized automated metrics.\n",
        "- Merge the two aggregated DataFrames on QuestionID.\n",
        "- Define metric pairs for comparison (survey vs. automated).\n",
        "- Split into Overall, Non-sensitive, and Sensitive subsets.\n",
        "- For each subset and metric pair, compute and print Pearson r and p-value.\n",
        "\"\"\"\n",
        "\n",
        "# Aggregate survey standardized metrics by question (take the mean over responses)\n",
        "survey_agg = valid_df.groupby(\"QuestionID\", as_index=False).agg({\n",
        "    \"EnglishNeutrality_old\": \"mean\",\n",
        "    \"ArabicNeutrality_old\": \"mean\",\n",
        "    \"SentimentConsistency_old\": \"mean\",\n",
        "    \"Sensitivity\": \"first\"  # assuming sensitivity is consistent per question\n",
        "})\n",
        "\n",
        "# Aggregate automated standardized metrics by question (mean across model runs)\n",
        "auto_agg = df_auto.groupby(\"QuestionID\", as_index=False).agg({\n",
        "    \"NeutralScore_en_std\": \"mean\",\n",
        "    \"NeutralScore_ar_std\": \"mean\",\n",
        "    \"AutoConsistency_old\": \"mean\"\n",
        "})\n",
        "\n",
        "# Merge the aggregated data on QuestionID\n",
        "merged_std = pd.merge(survey_agg, auto_agg, on=\"QuestionID\", how=\"inner\")\n",
        "\n",
        "# Define the metric pairs for correlation analysis\n",
        "metrics = {\n",
        "    \"English Neutrality\": (\"EnglishNeutrality_old\", \"NeutralScore_en_std\"),\n",
        "    \"Arabic Neutrality\": (\"ArabicNeutrality_old\", \"NeutralScore_ar_std\"),\n",
        "    \"Sentiment Consistency\": (\"SentimentConsistency_old\", \"AutoConsistency_old\")\n",
        "}\n",
        "\n",
        "# Define the data subsets\n",
        "subsets = {\n",
        "    \"Overall\": merged_std,\n",
        "    \"Non-sensitive\": merged_std[merged_std[\"Sensitivity\"] == \"Non-sensitive\"],\n",
        "    \"Sensitive\": merged_std[merged_std[\"Sensitivity\"] == \"Sensitive\"]\n",
        "}\n",
        "\n",
        "# Compute and print correlations for each metric pair in each subset\n",
        "for subset_name, df_subset in subsets.items():\n",
        "    print(f\"=== {subset_name} Data ===\")\n",
        "    # Check if there is enough data to compute correlations\n",
        "    if len(df_subset) < 2:\n",
        "        print(\"Not enough data for correlation analysis in this subset.\\n\")\n",
        "        continue\n",
        "    for metric_name, (survey_col, auto_col) in metrics.items():\n",
        "        print(f\"--- {metric_name} ---\")\n",
        "        # Pearson correlation\n",
        "        pear_r, pear_p = pearsonr(df_subset[survey_col], df_subset[auto_col])\n",
        "        print(f\"Pearson r: {pear_r:.3f} (p = {pear_p:.4g})\")\n",
        "\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "qcHQ3yhUZ3dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Aggregate and compare survey and automated metrics by question, generate ranking tables,\n",
        "and compute Spearman's rank correlations for specified metrics.\n",
        "\n",
        "Defines:\n",
        "- rbo_ext: compute extrapolated Rank-Biased Overlap between two ranked lists.\n",
        "- run_ranking_analysis: aggregate data, produce ranking tables, and compute Spearman correlations.\n",
        "\n",
        "Example analyses at the bottom cover:\n",
        "1. Sentiment consistency\n",
        "2. Arabic neutrality\n",
        "3. English neutrality\n",
        "\"\"\"\n",
        "\n",
        "def rbo_ext(S, T, p=0.9):\n",
        "    \"\"\"\n",
        "    Compute the extrapolated Rank-Biased Overlap (RBO_ext) between two ranked lists.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    S : list\n",
        "        First ranked list of items.\n",
        "    T : list\n",
        "        Second ranked list of items.\n",
        "    p : float, optional\n",
        "        Persistence parameter (0 < p < 1) weighting top ranks more heavily.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Extrapolated RBO value between 0 and 1.\n",
        "    \"\"\"\n",
        "    k = max(len(S), len(T))\n",
        "    sum_overlap = 0.0\n",
        "    set_S, set_T = set(), set()\n",
        "    for d in range(1, k + 1):\n",
        "        if d <= len(S):\n",
        "            set_S.add(S[d - 1])\n",
        "        if d <= len(T):\n",
        "            set_T.add(T[d - 1])\n",
        "        overlap = len(set_S & set_T)\n",
        "        sum_overlap += (p ** (d - 1)) * (overlap / d)\n",
        "    return (1 - p) * sum_overlap + (p ** k)\n",
        "\n",
        "def run_ranking_analysis(df_auto, valid_df, survey_col, auto_col, survey_label, auto_label):\n",
        "    \"\"\"\n",
        "    Aggregate survey and automated results by QuestionID, generate ranking tables,\n",
        "    and compute Spearman's rank correlations for the specified metrics.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_auto : pandas.DataFrame\n",
        "        Automated analysis data with 'QuestionID' and 'Sensitivity'.\n",
        "    valid_df : pandas.DataFrame\n",
        "        Survey responses with 'QuestionID', 'Sensitivity', and survey_col.\n",
        "    survey_col : str\n",
        "        Name of the survey metric column in valid_df.\n",
        "    auto_col : str\n",
        "        Name of the automated metric column in df_auto.\n",
        "    survey_label : str\n",
        "        Label for the aggregated survey metric in output tables.\n",
        "    auto_label : str\n",
        "        Label for the aggregated automated metric in output tables.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        Prints aggregated data, ranking tables, and Spearman correlation results.\n",
        "    \"\"\"\n",
        "    # Aggregate automated data\n",
        "    df_auto_agg = df_auto.groupby(\"QuestionID\", as_index=False).agg({\n",
        "        auto_col: \"mean\",\n",
        "        \"Sensitivity\": \"first\"\n",
        "    })\n",
        "\n",
        "    # Aggregate survey data\n",
        "    df_survey_agg = valid_df.groupby(\"QuestionID\", as_index=False).agg({\n",
        "        survey_col: \"mean\",\n",
        "        \"Sensitivity\": \"first\"\n",
        "    }).rename(columns={survey_col: survey_label})\n",
        "\n",
        "    print(\"Aggregated Automated Data:\")\n",
        "    print(df_auto_agg)\n",
        "    print(\"\\nAggregated Survey Data:\")\n",
        "    print(df_survey_agg)\n",
        "\n",
        "    # Overall ranking\n",
        "    survey_rank = df_survey_agg.sort_values(by=survey_label, ascending=False).reset_index(drop=True)\n",
        "    survey_rank[\"Rank\"] = survey_rank.index + 1\n",
        "    survey_rank = survey_rank.rename(columns={\"QuestionID\": \"SurveyQuestionID\"})\n",
        "    auto_rank = df_auto_agg.sort_values(by=auto_col, ascending=False).reset_index(drop=True)\n",
        "    auto_rank[\"Rank\"] = auto_rank.index + 1\n",
        "    auto_rank = auto_rank.rename(columns={\"QuestionID\": \"AutoQuestionID\"})\n",
        "    overall_ranking = pd.merge(\n",
        "        survey_rank[[\"Rank\", \"SurveyQuestionID\", survey_label, \"Sensitivity\"]],\n",
        "        auto_rank[[\"Rank\", \"AutoQuestionID\", auto_col]],\n",
        "        on=\"Rank\"\n",
        "    )\n",
        "    print(\"\\nOverall Ranking Table (Rank 1 = highest):\")\n",
        "    print(overall_ranking)\n",
        "    overall_rho, overall_p = spearmanr(df_survey_agg[survey_label], df_auto_agg[auto_col])\n",
        "    print(f\"\\nOverall Spearman's Rank Correlation: rho = {overall_rho:.3f}, p = {overall_p:.4g}\")\n",
        "\n",
        "    # Non-sensitive ranking\n",
        "    df_survey_non = df_survey_agg[df_survey_agg[\"Sensitivity\"] == \"Non-sensitive\"].copy()\n",
        "    df_auto_non   = df_auto_agg[df_auto_agg[\"Sensitivity\"] == \"Non-sensitive\"].copy()\n",
        "    survey_rank_non = df_survey_non.sort_values(by=survey_label, ascending=False).reset_index(drop=True)\n",
        "    survey_rank_non[\"Rank\"] = survey_rank_non.index + 1\n",
        "    survey_rank_non = survey_rank_non.rename(columns={\"QuestionID\": \"SurveyQuestionID\"})\n",
        "    auto_rank_non = df_auto_non.sort_values(by=auto_col, ascending=False).reset_index(drop=True)\n",
        "    auto_rank_non[\"Rank\"] = auto_rank_non.index + 1\n",
        "    auto_rank_non = auto_rank_non.rename(columns={\"QuestionID\": \"AutoQuestionID\"})\n",
        "    nonsensitive_ranking = pd.merge(\n",
        "        survey_rank_non[[\"Rank\", \"SurveyQuestionID\", survey_label, \"Sensitivity\"]],\n",
        "        auto_rank_non[[\"Rank\", \"AutoQuestionID\", auto_col]],\n",
        "        on=\"Rank\"\n",
        "    )\n",
        "    print(\"\\nNon-sensitive Ranking Table:\")\n",
        "    print(nonsensitive_ranking)\n",
        "    if len(df_survey_non) > 1 and len(df_auto_non) > 1:\n",
        "        nonsens_rho, nonsens_p = spearmanr(df_survey_non[survey_label], df_auto_non[auto_col])\n",
        "        print(f\"\\nNon-sensitive Spearman's Rank Correlation: rho = {nonsens_rho:.3f}, p = {nonsens_p:.4g}\")\n",
        "    else:\n",
        "        print(\"\\nNot enough non-sensitive data for Spearman correlation.\")\n",
        "\n",
        "    # Sensitive ranking\n",
        "    df_survey_sens = df_survey_agg[df_survey_agg[\"Sensitivity\"] == \"Sensitive\"].copy()\n",
        "    df_auto_sens   = df_auto_agg[df_auto_agg[\"Sensitivity\"] == \"Sensitive\"].copy()\n",
        "    survey_rank_sens = df_survey_sens.sort_values(by=survey_label, ascending=False).reset_index(drop=True)\n",
        "    survey_rank_sens[\"Rank\"] = survey_rank_sens.index + 1\n",
        "    survey_rank_sens = survey_rank_sens.rename(columns={\"QuestionID\": \"SurveyQuestionID\"})\n",
        "    auto_rank_sens = df_auto_sens.sort_values(by=auto_col, ascending=False).reset_index(drop=True)\n",
        "    auto_rank_sens[\"Rank\"] = auto_rank_sens.index + 1\n",
        "    auto_rank_sens = auto_rank_sens.rename(columns={\"QuestionID\": \"AutoQuestionID\"})\n",
        "    sensitive_ranking = pd.merge(\n",
        "        survey_rank_sens[[\"Rank\", \"SurveyQuestionID\", survey_label, \"Sensitivity\"]],\n",
        "        auto_rank_sens[[\"Rank\", \"AutoQuestionID\", auto_col]],\n",
        "        on=\"Rank\"\n",
        "    )\n",
        "    print(\"\\nSensitive Ranking Table:\")\n",
        "    print(sensitive_ranking)\n",
        "    if len(df_survey_sens) > 1 and len(df_auto_sens) > 1:\n",
        "        sens_rho, sens_p = spearmanr(df_survey_sens[survey_label], df_auto_sens[auto_col])\n",
        "        print(f\"\\nSensitive Spearman's Rank Correlation: rho = {sens_rho:.3f}, p = {sens_p:.4g}\")\n",
        "    else:\n",
        "        print(\"\\nNot enough sensitive data for Spearman correlation.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=== Analysis for Sentiment Consistency ===\")\n",
        "    run_ranking_analysis(\n",
        "        df_auto, valid_df,\n",
        "        survey_col=\"SentimentConsistency\", auto_col=\"AutoConsistency\",\n",
        "        survey_label=\"SurveyConsistency\", auto_label=\"AutoConsistency\"\n",
        "    )\n",
        "    print(\"\\n=== Analysis for Arabic Neutrality ===\")\n",
        "    run_ranking_analysis(\n",
        "        df_auto, valid_df,\n",
        "        survey_col=\"ArabicNeutrality\", auto_col=\"NeutralScore_ar\",\n",
        "        survey_label=\"SurveyArabicNeutrality\", auto_label=\"AutoNeutral_ar\"\n",
        "    )\n",
        "    print(\"\\n=== Analysis for English Neutrality ===\")\n",
        "    run_ranking_analysis(\n",
        "        df_auto, valid_df,\n",
        "        survey_col=\"EnglishNeutrality\", auto_col=\"NeutralScore_en\",\n",
        "        survey_label=\"SurveyEnglishNeutrality\", auto_label=\"AutoNeutral_en\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "XJvYQfD-amwa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}